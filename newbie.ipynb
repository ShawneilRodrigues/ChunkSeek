{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                                 Version            Editable project location\n",
      "--------------------------------------- ------------------ ------------------------------------------------------------------------\n",
      "absl-py                                 2.1.0\n",
      "aext-assistant                          4.0.15\n",
      "aext-assistant-server                   4.0.15\n",
      "aext-core                               4.0.15\n",
      "aext-core-server                        4.0.15\n",
      "aext-panels                             4.0.15\n",
      "aext-panels-server                      4.0.15\n",
      "aext-share-notebook                     4.0.15\n",
      "aext-share-notebook-server              4.0.15\n",
      "aext-shared                             4.0.15\n",
      "aiobotocore                             2.12.3\n",
      "aiohappyeyeballs                        2.3.6\n",
      "aiohttp                                 3.10.3\n",
      "aioice                                  0.9.0\n",
      "aioitertools                            0.7.1\n",
      "aiortc                                  1.9.0\n",
      "aiosignal                               1.3.1\n",
      "alabaster                               0.7.16\n",
      "alembic                                 1.13.3\n",
      "altair                                  5.3.0\n",
      "anaconda-anon-usage                     0.4.4\n",
      "anaconda-catalogs                       0.2.0\n",
      "anaconda-client                         1.12.3\n",
      "anaconda-cloud-auth                     0.5.1\n",
      "anaconda-navigator                      2.6.0\n",
      "anaconda-project                        0.11.1\n",
      "annotated-types                         0.6.0\n",
      "anyio                                   4.3.0\n",
      "anytree                                 2.12.1\n",
      "appdirs                                 1.4.4\n",
      "archspec                                0.2.3\n",
      "argon2-cffi                             23.1.0\n",
      "argon2-cffi-bindings                    21.2.0\n",
      "arrow                                   1.3.0\n",
      "arxiv                                   2.1.3\n",
      "astroid                                 2.14.2\n",
      "astropy                                 6.1.0\n",
      "astropy-iers-data                       0.2024.6.3.0.31.14\n",
      "asttokens                               2.4.1\n",
      "astunparse                              1.6.3\n",
      "async-lru                               2.0.4\n",
      "atomicwrites                            1.4.0\n",
      "attrs                                   23.2.0\n",
      "Automat                                 20.2.0\n",
      "autopep8                                2.0.4\n",
      "av                                      12.3.0\n",
      "Babel                                   2.14.0\n",
      "bcrypt                                  3.2.0\n",
      "beautifulsoup4                          4.12.3\n",
      "binaryornot                             0.4.4\n",
      "black                                   24.4.2\n",
      "bleach                                  6.1.0\n",
      "blinker                                 1.7.0\n",
      "bokeh                                   3.4.1\n",
      "boltons                                 23.0.0\n",
      "boto3                                   1.34.162\n",
      "botocore                                1.34.162\n",
      "Bottleneck                              1.3.7\n",
      "branca                                  0.8.1\n",
      "Brotli                                  1.1.0\n",
      "cachetools                              5.4.0\n",
      "certifi                                 2024.12.14\n",
      "cffi                                    1.16.0\n",
      "chardet                                 4.0.0\n",
      "charset-normalizer                      3.3.2\n",
      "click                                   8.1.7\n",
      "cloudpickle                             2.2.1\n",
      "cmdstanpy                               1.2.5\n",
      "cohere                                  5.11.0\n",
      "colorama                                0.4.6\n",
      "colorcet                                3.1.0\n",
      "comm                                    0.2.2\n",
      "conda                                   24.7.1\n",
      "conda-build                             24.5.1\n",
      "conda-content-trust                     0.2.0\n",
      "conda_index                             0.5.0\n",
      "conda-libmamba-solver                   24.1.0\n",
      "conda-pack                              0.7.1\n",
      "conda-package-handling                  2.3.0\n",
      "conda_package_streaming                 0.10.0\n",
      "conda-repo-cli                          1.0.88\n",
      "conda-token                             0.5.0+1.g2209e04\n",
      "constantly                              23.10.4\n",
      "contourpy                               1.2.1\n",
      "cookiecutter                            2.6.0\n",
      "crypto                                  1.4.1\n",
      "cryptography                            42.0.5\n",
      "cssselect                               1.2.0\n",
      "cycler                                  0.12.1\n",
      "cytoolz                                 0.12.2\n",
      "dask                                    2024.5.0\n",
      "dask-expr                               1.1.0\n",
      "databricks-sdk                          0.35.0\n",
      "dataclasses-json                        0.6.7\n",
      "datasets                                2.21.0\n",
      "datashader                              0.16.2\n",
      "debugpy                                 1.8.1\n",
      "decorator                               4.4.2\n",
      "defusedxml                              0.7.1\n",
      "Deprecated                              1.2.14\n",
      "deprecation                             2.1.0\n",
      "diff-match-patch                        20200713\n",
      "dill                                    0.3.9\n",
      "dirtyjson                               1.0.8\n",
      "distlib                                 0.3.8\n",
      "distributed                             2024.5.0\n",
      "distro                                  1.9.0\n",
      "dnspython                               2.7.0\n",
      "docker                                  7.1.0\n",
      "docstring_parser                        0.16\n",
      "docstring-to-markdown                   0.11\n",
      "docutils                                0.18.1\n",
      "duckdb                                  1.1.3\n",
      "duckduckgo_search                       7.1.1\n",
      "editdistance                            0.8.1\n",
      "efficientnet                            1.0.0\n",
      "elevenlabs                              1.11.0\n",
      "ensure                                  1.0.2\n",
      "entrypoints                             0.4\n",
      "essential-generators                    1.0\n",
      "et-xmlfile                              1.1.0\n",
      "executing                               2.0.1\n",
      "faiss-cpu                               1.9.0.post1\n",
      "Farama-Notifications                    0.0.4\n",
      "fastapi                                 0.78.0\n",
      "fastavro                                1.9.7\n",
      "fastjsonschema                          2.19.1\n",
      "feedparser                              6.0.11\n",
      "ffmpeg                                  1.4\n",
      "filelock                                3.13.3\n",
      "filetype                                1.2.0\n",
      "flake8                                  7.0.0\n",
      "Flask                                   3.0.2\n",
      "Flask-Cors                              5.0.0\n",
      "flatbuffers                             24.3.25\n",
      "folium                                  0.19.4\n",
      "fonttools                               4.50.0\n",
      "fqdn                                    1.5.1\n",
      "frozendict                              2.4.2\n",
      "frozenlist                              1.4.1\n",
      "fsspec                                  2024.6.1\n",
      "gast                                    0.6.0\n",
      "gensim                                  4.3.2\n",
      "geographiclib                           2.0\n",
      "geopy                                   2.4.1\n",
      "gitdb                                   4.0.11\n",
      "GitPython                               3.1.43\n",
      "google-ai-generativelanguage            0.6.10\n",
      "google-api-core                         2.24.0\n",
      "google-api-python-client                2.157.0\n",
      "google-auth                             2.35.0\n",
      "google-auth-httplib2                    0.2.0\n",
      "google-cloud-aiplatform                 1.77.0\n",
      "google-cloud-bigquery                   3.27.0\n",
      "google-cloud-core                       2.4.1\n",
      "google-cloud-resource-manager           1.14.0\n",
      "google-cloud-storage                    2.19.0\n",
      "google-crc32c                           1.6.0\n",
      "google-generativeai                     0.8.3\n",
      "google-pasta                            0.2.0\n",
      "google-resumable-media                  2.7.2\n",
      "googleapis-common-protos                1.66.0\n",
      "graphene                                3.4\n",
      "graphql-core                            3.2.5\n",
      "graphql-relay                           3.2.0\n",
      "graphviz                                0.20.3\n",
      "greenlet                                3.0.1\n",
      "groq                                    0.13.1\n",
      "grpc-google-iam-v1                      0.14.0\n",
      "grpcio                                  1.69.0\n",
      "grpcio-status                           1.69.0\n",
      "gTTS                                    2.5.3\n",
      "gym                                     0.26.2\n",
      "gym-notices                             0.0.8\n",
      "gymnasium                               0.29.1\n",
      "h11                                     0.14.0\n",
      "h5py                                    3.11.0\n",
      "HeapDict                                1.0.1\n",
      "holidays                                0.65\n",
      "holoviews                               1.19.0\n",
      "html5lib                                1.1\n",
      "httpcore                                1.0.5\n",
      "httplib2                                0.22.0\n",
      "httpx                                   0.27.0\n",
      "httpx-sse                               0.4.0\n",
      "huggingface-hub                         0.24.5\n",
      "hvplot                                  0.10.0\n",
      "hyperlink                               21.0.0\n",
      "idna                                    3.6\n",
      "ifaddr                                  0.2.0\n",
      "imagecodecs                             2023.1.23\n",
      "imageio                                 2.35.1\n",
      "imageio-ffmpeg                          0.5.1\n",
      "imagesize                               1.4.1\n",
      "imbalanced-learn                        0.12.3\n",
      "imgaug                                  0.4.0\n",
      "importlib-metadata                      6.11.0\n",
      "importlib_resources                     6.5.2\n",
      "incremental                             22.10.0\n",
      "inflate64                               1.0.0\n",
      "inflection                              0.5.1\n",
      "iniconfig                               1.1.1\n",
      "intake                                  0.7.0\n",
      "intervaltree                            3.1.0\n",
      "ipykernel                               6.29.4\n",
      "ipython                                 8.23.0\n",
      "ipython-genutils                        0.2.0\n",
      "ipywidgets                              8.1.2\n",
      "isoduration                             20.11.0\n",
      "isort                                   5.13.2\n",
      "itemadapter                             0.3.0\n",
      "itemloaders                             1.1.0\n",
      "itsdangerous                            2.1.2\n",
      "jaraco.classes                          3.2.1\n",
      "jedi                                    0.19.1\n",
      "jellyfish                               1.0.1\n",
      "Jinja2                                  3.1.2\n",
      "jiter                                   0.6.1\n",
      "jmespath                                1.0.1\n",
      "joblib                                  1.4.2\n",
      "json5                                   0.9.24\n",
      "jsonpatch                               1.33\n",
      "jsonpickle                              3.3.0\n",
      "jsonpointer                             2.4\n",
      "jsonschema                              4.21.1\n",
      "jsonschema-specifications               2023.12.1\n",
      "jupyter                                 1.0.0\n",
      "jupyter_client                          8.6.1\n",
      "jupyter-console                         6.6.3\n",
      "jupyter_core                            5.7.2\n",
      "jupyter-events                          0.10.0\n",
      "jupyter-lsp                             2.2.4\n",
      "jupyter_server                          2.13.0\n",
      "jupyter_server_terminals                0.5.3\n",
      "jupyterlab                              4.1.5\n",
      "jupyterlab_pygments                     0.3.0\n",
      "jupyterlab_server                       2.25.4\n",
      "jupyterlab_widgets                      3.0.10\n",
      "jwt                                     1.3.1\n",
      "kaggle                                  1.6.14\n",
      "kaleido                                 0.2.1\n",
      "keras                                   3.6.0\n",
      "Keras-Applications                      1.0.8\n",
      "keras-ocr                               0.9.3\n",
      "keyring                                 24.3.1\n",
      "kiwisolver                              1.4.5\n",
      "lancedb                                 0.15.0\n",
      "langchain                               0.3.14\n",
      "langchain-cohere                        0.3.1\n",
      "langchain-community                     0.3.14\n",
      "langchain-core                          0.3.30\n",
      "langchain-experimental                  0.3.2\n",
      "langchain-google-genai                  2.0.9\n",
      "langchain-text-splitters                0.3.5\n",
      "langsmith                               0.1.136\n",
      "lazy_loader                             0.4\n",
      "lazy-object-proxy                       1.10.0\n",
      "lckr_jupyterlab_variableinspector       3.1.0\n",
      "libarchive-c                            2.9\n",
      "libclang                                18.1.1\n",
      "libmambapy                              1.5.8\n",
      "lime                                    0.2.0.1\n",
      "linkify-it-py                           2.0.0\n",
      "llama-cloud                             0.1.8\n",
      "llama-index                             0.12.11\n",
      "llama-index-agent-openai                0.4.1\n",
      "llama-index-cli                         0.4.0\n",
      "llama-index-core                        0.12.12\n",
      "llama-index-embeddings-huggingface      0.5.0\n",
      "llama-index-embeddings-openai           0.3.1\n",
      "llama-index-indices-managed-llama-cloud 0.6.3\n",
      "llama-index-llms-gemini                 0.4.4\n",
      "llama-index-llms-ollama                 0.5.0\n",
      "llama-index-llms-openai                 0.3.13\n",
      "llama-index-multi-modal-llms-openai     0.4.2\n",
      "llama-index-program-openai              0.3.1\n",
      "llama-index-question-gen-openai         0.3.0\n",
      "llama-index-readers-file                0.4.3\n",
      "llama-index-readers-llama-parse         0.4.0\n",
      "llama-index-vector-stores-objectbox     0.1.0a0\n",
      "llama-parse                             0.5.19\n",
      "llvmlite                                0.42.0\n",
      "lmdb                                    1.4.1\n",
      "locket                                  1.0.0\n",
      "lxml                                    5.3.0\n",
      "lxml_html_clean                         0.4.1\n",
      "lz4                                     4.3.2\n",
      "Mako                                    1.3.5\n",
      "Markdown                                3.6\n",
      "markdown-it-py                          3.0.0\n",
      "MarkupSafe                              2.1.5\n",
      "marshmallow                             3.23.0\n",
      "matplotlib                              3.8.3\n",
      "matplotlib-inline                       0.1.6\n",
      "mccabe                                  0.7.0\n",
      "mdit-py-plugins                         0.3.0\n",
      "mdurl                                   0.1.2\n",
      "menuinst                                2.1.1\n",
      "minijinja                               2.6.0\n",
      "mistune                                 3.0.2\n",
      "mkl-fft                                 1.3.8\n",
      "mkl-random                              1.2.4\n",
      "mkl-service                             2.4.0\n",
      "ml-dtypes                               0.4.0\n",
      "mlflow                                  2.17.0\n",
      "mlflow-skinny                           2.17.0\n",
      "mock                                    4.0.3\n",
      "more-itertools                          10.1.0\n",
      "moviepy                                 1.0.3\n",
      "mpmath                                  1.3.0\n",
      "msgpack                                 1.0.3\n",
      "multidict                               6.0.5\n",
      "multipledispatch                        0.6.0\n",
      "multiprocess                            0.70.17\n",
      "multitasking                            0.0.11\n",
      "multivolumefile                         0.2.3\n",
      "mypy                                    1.10.0\n",
      "mypy-boto3-s3                           1.34.162\n",
      "mypy-extensions                         1.0.0\n",
      "Naked                                   0.1.32\n",
      "namex                                   0.0.8\n",
      "navigator-updater                       0.5.1\n",
      "nbclient                                0.10.0\n",
      "nbconvert                               7.16.3\n",
      "nbformat                                5.10.3\n",
      "neo4j                                   5.25.0\n",
      "nest-asyncio                            1.6.0\n",
      "networkx                                3.4.1\n",
      "newspaper4k                             0.9.3.1\n",
      "nltk                                    3.9.1\n",
      "notebook                                7.1.2\n",
      "notebook_shim                           0.2.4\n",
      "numba                                   0.59.1\n",
      "numexpr                                 2.8.7\n",
      "numpy                                   1.26.4\n",
      "numpydoc                                1.7.0\n",
      "oauthlib                                3.2.2\n",
      "objectbox                               4.0.0\n",
      "ollama                                  0.4.4\n",
      "openai                                  1.59.6\n",
      "opencv-python                           4.10.0.84\n",
      "opendatasets                            0.1.22\n",
      "openpyxl                                3.1.2\n",
      "opentelemetry-api                       1.27.0\n",
      "opentelemetry-sdk                       1.27.0\n",
      "opentelemetry-semantic-conventions      0.48b0\n",
      "opt-einsum                              3.3.0\n",
      "optree                                  0.12.1\n",
      "orjson                                  3.10.7\n",
      "overrides                               7.7.0\n",
      "packaging                               24.0\n",
      "pandas                                  2.2.1\n",
      "pandas-datareader                       0.10.0\n",
      "pandas_ta                               0.3.14b0\n",
      "pandocfilters                           1.5.1\n",
      "panel                                   1.4.4\n",
      "param                                   2.1.0\n",
      "parameterized                           0.9.0\n",
      "paramiko                                2.8.1\n",
      "parsel                                  1.8.1\n",
      "parso                                   0.8.3\n",
      "partd                                   1.4.1\n",
      "pathos                                  0.3.3\n",
      "pathspec                                0.10.3\n",
      "patsy                                   0.5.6\n",
      "peewee                                  3.17.7\n",
      "pexpect                                 4.8.0\n",
      "phi                                     0.6.7\n",
      "phidata                                 2.7.5\n",
      "pickleshare                             0.7.5\n",
      "pillow                                  10.3.0\n",
      "pinecone-client                         5.0.1\n",
      "pinecone-plugin-inference               1.1.0\n",
      "pinecone-plugin-interface               0.0.7\n",
      "pip                                     24.3.1\n",
      "pkce                                    1.0.3\n",
      "pkginfo                                 1.10.0\n",
      "platformdirs                            4.2.0\n",
      "plotly                                  5.20.0\n",
      "pluggy                                  1.0.0\n",
      "ply                                     3.11\n",
      "portalocker                             2.10.1\n",
      "pox                                     0.3.5\n",
      "ppft                                    1.7.6.9\n",
      "predictionguard                         2.5.0\n",
      "primp                                   0.9.2\n",
      "proglog                                 0.1.10\n",
      "prometheus_client                       0.20.0\n",
      "prompt-toolkit                          3.0.43\n",
      "prophet                                 1.1.6\n",
      "Protego                                 0.1.16\n",
      "proto-plus                              1.25.0\n",
      "protobuf                                5.29.2\n",
      "psutil                                  5.9.8\n",
      "ptyprocess                              0.7.0\n",
      "pure-eval                               0.2.2\n",
      "py-cpuinfo                              9.0.0\n",
      "py7zr                                   0.22.0\n",
      "pyarrow                                 16.1.0\n",
      "pyasn1                                  0.4.8\n",
      "pyasn1-modules                          0.2.8\n",
      "pybcj                                   1.0.2\n",
      "pyclipper                               1.3.0.post6\n",
      "pycodestyle                             2.11.1\n",
      "pycosat                                 0.6.6\n",
      "pycparser                               2.22\n",
      "pycryptodome                            3.21.0\n",
      "pycryptodomex                           3.20.0\n",
      "pyct                                    0.5.0\n",
      "pycurl                                  7.45.2\n",
      "pydantic                                2.10.5\n",
      "pydantic_core                           2.27.2\n",
      "pydantic-settings                       2.6.0\n",
      "pydeck                                  0.9.1\n",
      "PyDispatcher                            2.0.5\n",
      "pydocstyle                              6.3.0\n",
      "pyee                                    12.1.1\n",
      "pyerfa                                  2.0.1.4\n",
      "pyflakes                                3.2.0\n",
      "pygame                                  2.6.0\n",
      "PyGithub                                2.4.0\n",
      "Pygments                                2.17.2\n",
      "PyJWT                                   2.8.0\n",
      "pylance                                 0.19.1\n",
      "pylibsrtp                               0.10.0\n",
      "pylint                                  2.16.2\n",
      "pylint-venv                             3.0.3\n",
      "pyls-spyder                             0.4.0\n",
      "PyNaCl                                  1.5.0\n",
      "pyodbc                                  5.0.1\n",
      "pyOpenSSL                               24.0.0\n",
      "pyparsing                               3.1.2\n",
      "pypdf                                   5.1.0\n",
      "pyppmd                                  1.1.0\n",
      "PyQt5                                   5.15.10\n",
      "PyQt5-sip                               12.13.0\n",
      "PyQtWebEngine                           5.15.6\n",
      "PySocks                                 1.7.1\n",
      "pytesseract                             0.3.13\n",
      "pytest                                  7.4.4\n",
      "python-box                              6.0.2\n",
      "python-dateutil                         2.9.0.post0\n",
      "python-dotenv                           0.21.0\n",
      "python-json-logger                      2.0.7\n",
      "python-lsp-black                        2.0.0\n",
      "python-lsp-jsonrpc                      1.1.2\n",
      "python-lsp-server                       1.10.0\n",
      "python-slugify                          8.0.4\n",
      "python-snappy                           0.6.1\n",
      "pytoolconfig                            1.2.6\n",
      "pytube                                  15.0.0\n",
      "pytubefix                               8.12.0\n",
      "pytz                                    2024.1\n",
      "pyvis                                   0.3.2\n",
      "pyviz_comms                             3.0.2\n",
      "pywavelets                              1.5.0\n",
      "pywin32                                 306\n",
      "pywin32-ctypes                          0.2.2\n",
      "pywinpty                                2.0.13\n",
      "PyYAML                                  6.0.1\n",
      "pyzmq                                   25.1.2\n",
      "pyzstd                                  0.16.1\n",
      "QDarkStyle                              3.2.3\n",
      "qstylizer                               0.2.2\n",
      "QtAwesome                               1.2.2\n",
      "qtconsole                               5.5.1\n",
      "QtPy                                    2.4.1\n",
      "queuelib                                1.6.2\n",
      "referencing                             0.34.0\n",
      "regex                                   2024.7.24\n",
      "requests                                2.32.3\n",
      "requests-file                           1.5.1\n",
      "requests-oauthlib                       1.3.1\n",
      "requests-toolbelt                       1.0.0\n",
      "rfc3339-validator                       0.1.4\n",
      "rfc3986-validator                       0.1.1\n",
      "rich                                    13.7.1\n",
      "rope                                    1.12.0\n",
      "rouge_score                             0.1.2\n",
      "rpds-py                                 0.18.0\n",
      "rsa                                     4.9\n",
      "Rtree                                   1.0.1\n",
      "ruamel.yaml                             0.17.21\n",
      "ruamel-yaml-conda                       0.17.21\n",
      "s3fs                                    2024.3.1\n",
      "s3transfer                              0.10.2\n",
      "sacrebleu                               2.4.2\n",
      "safetensors                             0.4.4\n",
      "sagemaker                               2.232.2\n",
      "sagemaker-core                          1.0.10\n",
      "sagemaker-mlflow                        0.1.0\n",
      "schema                                  0.7.7\n",
      "scikit-image                            0.24.0\n",
      "scikit-learn                            1.4.2\n",
      "scipy                                   1.13.0\n",
      "Scrapy                                  2.11.1\n",
      "seaborn                                 0.13.2\n",
      "semver                                  3.0.2\n",
      "Send2Trash                              1.8.2\n",
      "sentence-transformers                   3.2.1\n",
      "sentencepiece                           0.2.0\n",
      "service-identity                        18.1.0\n",
      "setuptools                              75.8.0\n",
      "sgmllib3k                               1.0.0\n",
      "shap                                    0.46.0\n",
      "shapely                                 2.0.6\n",
      "shellescape                             3.8.1\n",
      "shellingham                             1.5.4\n",
      "sip                                     6.7.12\n",
      "six                                     1.16.0\n",
      "slicer                                  0.0.8\n",
      "smart-open                              5.2.1\n",
      "smdebug-rulesconfig                     1.0.1\n",
      "smmap                                   5.0.1\n",
      "sniffio                                 1.3.1\n",
      "snowballstemmer                         2.2.0\n",
      "sortedcontainers                        2.4.0\n",
      "soupsieve                               2.5\n",
      "SpeechRecognition                       3.8.1\n",
      "Sphinx                                  7.3.7\n",
      "sphinxcontrib-applehelp                 1.0.2\n",
      "sphinxcontrib-devhelp                   1.0.2\n",
      "sphinxcontrib-htmlhelp                  2.0.0\n",
      "sphinxcontrib-jsmath                    1.0.1\n",
      "sphinxcontrib-qthelp                    1.0.3\n",
      "sphinxcontrib-serializinghtml           1.1.10\n",
      "spyder                                  5.5.1\n",
      "spyder-kernels                          2.5.0\n",
      "SQLAlchemy                              2.0.30\n",
      "sqlparse                                0.5.1\n",
      "stable_baselines3                       2.3.2\n",
      "stack-data                              0.6.3\n",
      "stanio                                  0.5.1\n",
      "starlette                               0.19.1\n",
      "statsmodels                             0.14.2\n",
      "streamlit                               1.37.0\n",
      "streamlit_folium                        0.24.0\n",
      "streamlit-webrtc                        0.47.9\n",
      "striprtf                                0.0.26\n",
      "sympy                                   1.13.2\n",
      "tables                                  3.9.2\n",
      "tabulate                                0.9.0\n",
      "tblib                                   1.7.0\n",
      "tenacity                                8.2.3\n",
      "tensorboard                             2.18.0\n",
      "tensorboard-data-server                 0.7.2\n",
      "tensorflow                              2.18.0\n",
      "tensorflow_intel                        2.18.0\n",
      "termcolor                               2.4.0\n",
      "terminado                               0.18.1\n",
      "Text-Summarizer-Project                 0.0.0              C:\\Users\\shawn\\OneDrive\\Documents\\ml project\\Text-Summarizer-Project\\src\n",
      "text-unidecode                          1.3\n",
      "textdistance                            4.2.1\n",
      "texttable                               1.7.0\n",
      "tf_keras                                2.18.0\n",
      "tf-keras-vis                            0.8.7\n",
      "threadpoolctl                           3.5.0\n",
      "three-merge                             0.1.1\n",
      "tifffile                                2024.8.30\n",
      "tiktoken                                0.8.0\n",
      "tinycss2                                1.2.1\n",
      "tldextract                              3.2.0\n",
      "tokenizers                              0.19.1\n",
      "toml                                    0.10.2\n",
      "tomli                                   2.0.1\n",
      "tomlkit                                 0.11.1\n",
      "toolz                                   0.12.1\n",
      "torch                                   2.4.1+cpu\n",
      "torchaudio                              2.4.1+cpu\n",
      "torchvision                             0.19.1+cpu\n",
      "tornado                                 6.4\n",
      "tqdm                                    4.66.4\n",
      "traitlets                               5.14.2\n",
      "transformers                            4.44.0\n",
      "truststore                              0.8.0\n",
      "tweepy                                  4.14.0\n",
      "Twisted                                 23.10.0\n",
      "twisted-iocpsupport                     1.0.2\n",
      "typer                                   0.15.1\n",
      "types-python-dateutil                   2.9.0.20240316\n",
      "types-requests                          2.32.0.20240914\n",
      "typing_extensions                       4.12.2\n",
      "typing-inspect                          0.9.0\n",
      "tzdata                                  2024.1\n",
      "uc-micro-py                             1.0.1\n",
      "ujson                                   5.10.0\n",
      "ultralytics                             8.3.5\n",
      "ultralytics-thop                        2.0.9\n",
      "unicodedata2                            15.1.0\n",
      "Unidecode                               1.2.0\n",
      "uri-template                            1.3.0\n",
      "uritemplate                             4.1.1\n",
      "urllib3                                 2.2.1\n",
      "uvicorn                                 0.18.3\n",
      "validators                              0.34.0\n",
      "virtualenv                              20.25.1\n",
      "w3lib                                   2.1.2\n",
      "waitress                                3.0.0\n",
      "watchdog                                4.0.1\n",
      "wcwidth                                 0.2.13\n",
      "webcolors                               1.13\n",
      "webencodings                            0.5.1\n",
      "websocket-client                        1.7.0\n",
      "websockets                              13.1\n",
      "webvtt-py                               0.5.1\n",
      "Werkzeug                                3.0.2\n",
      "whatthepatch                            1.0.2\n",
      "wheel                                   0.45.1\n",
      "widgetsnbextension                      4.0.10\n",
      "wikipedia                               1.4.0\n",
      "win-inet-pton                           1.1.0\n",
      "wisper                                  1.0.0\n",
      "wrapt                                   1.16.0\n",
      "xarray                                  2023.6.0\n",
      "xlwings                                 0.31.4\n",
      "xxhash                                  3.4.1\n",
      "xyzservices                             2022.9.0\n",
      "yapf                                    0.40.2\n",
      "yarl                                    1.9.4\n",
      "yfinance                                0.2.48\n",
      "youtube-transcript-api                  0.6.2\n",
      "yt-dlp                                  2025.1.26\n",
      "zict                                    3.0.0\n",
      "zipp                                    3.17.0\n",
      "zope.interface                          5.4.0\n",
      "zstandard                               0.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m chunk_audio \u001b[38;5;241m=\u001b[39m audio[start_ms:end_ms]\n\u001b[0;32m     70\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m \u001b[43mchunk_audio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydub\\audio_segment.py:963\u001b[0m, in \u001b[0;36mAudioSegment.export\u001b[1;34m(self, out_f, format, codec, bitrate, parameters, tags, id3v2_version, cover)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;66;03m# read stdin / write stdout\u001b[39;00m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mdevnull, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m devnull:\n\u001b[1;32m--> 963\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversion_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevnull\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m p_out, p_err \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[0;32m    966\u001b[0m log_subprocess_output(p_out)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import webvtt\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the file paths with direct paths\n",
    "vtt_file = \"c:/Users/shawn/OneDrive/Desktop/newbie/-uleG_Vecis.vtt\"\n",
    "audio_file = \"c:/Users/shawn/OneDrive/Desktop/newbie/100+ Computer Science Concepts Explained.wav\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"c:/Users/shawn/OneDrive/Desktop/newbie/chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(vtt_file):\n",
    "    print(f\"Looking for VTT file at: {vtt_file}\")\n",
    "    raise FileNotFoundError(f\"VTT file not found at: {vtt_file}\")\n",
    "\n",
    "# Load transcript with timestamps\n",
    "vtt = webvtt.read(vtt_file)\n",
    "if not os.path.exists(audio_file):\n",
    "    raise FileNotFoundError(f\"Audio file not found at: {audio_file}\")\n",
    "\n",
    "audio = AudioSegment.from_file(audio_file)\n",
    "\n",
    "# Create transcript dictionary\n",
    "transcript = {\n",
    "    \"segments\": [{\n",
    "        \"words\": [{\n",
    "            \"word\": caption.text,\n",
    "            \"start\": caption.start_in_seconds,\n",
    "            \"end\": caption.end_in_seconds  # Add end time\n",
    "        }]\n",
    "    } for caption in vtt.captions]\n",
    "}\n",
    "\n",
    "# Audio is already loaded above, remove this duplicate line\n",
    "chunks = []\n",
    "\n",
    "current_chunk = []\n",
    "current_start = 0\n",
    "\n",
    "for segment in transcript[\"segments\"]:\n",
    "    for word in segment[\"words\"]:\n",
    "        if word[\"end\"] - current_start > 15:  # Check if chunk exceeds 15 seconds\n",
    "            # Save current chunk\n",
    "            chunks.append({\n",
    "                \"text\": \" \".join([w[\"word\"] for w in current_chunk]),\n",
    "                \"start\": current_start,\n",
    "                \"end\": word[\"end\"]\n",
    "            })\n",
    "            # Start new chunk with overlap\n",
    "            current_chunk = current_chunk[-2:]  # Keep last 2 words for overlap\n",
    "            current_start = current_chunk[0][\"start\"]\n",
    "        current_chunk.append(word)\n",
    "\n",
    "# Save the last chunk\n",
    "if current_chunk:\n",
    "    chunks.append({\n",
    "        \"text\": \" \".join([w[\"word\"] for w in current_chunk]),\n",
    "        \"start\": current_start,\n",
    "        \"end\": current_chunk[-1][\"end\"]\n",
    "    })\n",
    "\n",
    "# Export audio chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    start_ms = chunk[\"start\"] * 1000\n",
    "    end_ms = chunk[\"end\"] * 1000\n",
    "    chunk_audio = audio[start_ms:end_ms]\n",
    "    output_path = os.path.join(output_dir, f\"chunk_{i}.mp3\")\n",
    "    chunk_audio.export(output_path, format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import whisperx\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import webrtcvad\n",
    "import wave\n",
    "import contextlib\n",
    "from scipy.io import wavfile\n",
    "import json\n",
    "\n",
    "class VideoSegmenter:\n",
    "    def __init__(self, max_segment_length=15):\n",
    "        self.max_segment_length = max_segment_length\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # Initialize VAD\n",
    "        self.vad = webrtcvad.Vad(3)  # Aggressiveness level 3\n",
    "        # Initialize sentence transformer for semantic similarity\n",
    "        self.sentence_analyzer = pipeline(\"feature-extraction\", \n",
    "                                        model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                        device=self.device)\n",
    "    \n",
    "    def download_video(self, url):\n",
    "        \"\"\"Download video and extract audio using yt-dlp\"\"\"\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'wav',\n",
    "                'preferredquality': '192',\n",
    "            }],\n",
    "            'outtmpl': 'audio.%(ext)s'\n",
    "        }\n",
    "        \n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([url])\n",
    "        return 'audio.wav'\n",
    "\n",
    "    def transcribe_audio(self, audio_path):\n",
    "        \"\"\"Transcribe audio using WhisperX for better alignment\"\"\"\n",
    "        # Load WhisperX model\n",
    "        model = whisperx.load_model(\"large-v2\", device=self.device)\n",
    "        \n",
    "        # Transcribe with word-level timestamps\n",
    "        result = model.transcribe(audio_path, batch_size=16)\n",
    "        \n",
    "        # Align whisper output\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=self.device)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio_path, device=self.device)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def detect_voice_activity(self, audio_path, window_size=30):\n",
    "        \"\"\"Detect voice activity in audio using WebRTC VAD\"\"\"\n",
    "        with contextlib.closing(wave.open(audio_path, 'rb')) as wf:\n",
    "            num_frames = wf.getnframes()\n",
    "            frame_rate = wf.getframerate()\n",
    "            audio_duration = num_frames / float(frame_rate)\n",
    "            \n",
    "        rate, audio = wavfile.read(audio_path)\n",
    "        samples_per_window = int(window_size * rate / 1000)\n",
    "        \n",
    "        voice_activity = []\n",
    "        for start in range(0, len(audio), samples_per_window):\n",
    "            is_speech = self.vad.is_speech(audio[start:start + samples_per_window].tobytes(), \n",
    "                                         sample_rate=rate)\n",
    "            voice_activity.append(is_speech)\n",
    "            \n",
    "        return voice_activity\n",
    "\n",
    "    def calculate_semantic_similarity(self, text1, text2):\n",
    "        \"\"\"Calculate semantic similarity between two text segments\"\"\"\n",
    "        embeddings1 = self.sentence_analyzer(text1)\n",
    "        embeddings2 = self.sentence_analyzer(text2)\n",
    "        \n",
    "        similarity = np.dot(embeddings1[0], embeddings2[0]) / \\\n",
    "                    (np.linalg.norm(embeddings1[0]) * np.linalg.norm(embeddings2[0]))\n",
    "        return similarity\n",
    "\n",
    "    def create_segments(self, transcription, voice_activity, audio_path):\n",
    "        \"\"\"Create semantically meaningful segments\"\"\"\n",
    "        segments = []\n",
    "        current_segment = {\n",
    "            'start': transcription['segments'][0]['start'],\n",
    "            'text': '',\n",
    "            'words': []\n",
    "        }\n",
    "        \n",
    "        for segment in transcription['segments']:\n",
    "            for word in segment['words']:\n",
    "                # Check if adding this word would exceed max duration\n",
    "                if word['end'] - current_segment['start'] > self.max_segment_length:\n",
    "                    # Finalize current segment\n",
    "                    current_segment['end'] = word['start']\n",
    "                    segments.append(current_segment)\n",
    "                    \n",
    "                    # Start new segment\n",
    "                    current_segment = {\n",
    "                        'start': word['start'],\n",
    "                        'text': '',\n",
    "                        'words': []\n",
    "                    }\n",
    "                \n",
    "                # Add word to current segment\n",
    "                current_segment['words'].append(word)\n",
    "                current_segment['text'] += ' ' + word['word']\n",
    "        \n",
    "        # Add final segment\n",
    "        if current_segment['words']:\n",
    "            current_segment['end'] = current_segment['words'][-1]['end']\n",
    "            segments.append(current_segment)\n",
    "        \n",
    "        # Post-process segments based on semantic similarity and voice activity\n",
    "        final_segments = self.optimize_segments(segments, voice_activity)\n",
    "        \n",
    "        return final_segments\n",
    "\n",
    "    def optimize_segments(self, segments, voice_activity):\n",
    "        \"\"\"Optimize segments based on semantic coherence and voice activity\"\"\"\n",
    "        optimized_segments = []\n",
    "        current_segment = segments[0]\n",
    "        \n",
    "        for i in range(1, len(segments)):\n",
    "            # Calculate semantic similarity with next segment\n",
    "            similarity = self.calculate_semantic_similarity(\n",
    "                current_segment['text'], \n",
    "                segments[i]['text']\n",
    "            )\n",
    "            \n",
    "            # Check voice activity at segment boundary\n",
    "            boundary_time = segments[i]['start']\n",
    "            va_index = int(boundary_time * 1000 / 30)  # Assuming 30ms windows\n",
    "            has_voice = voice_activity[va_index] if va_index < len(voice_activity) else False\n",
    "            \n",
    "            # Merge segments if they are semantically similar and have continuous voice activity\n",
    "            if similarity > 0.8 and has_voice:\n",
    "                current_segment['end'] = segments[i]['end']\n",
    "                current_segment['text'] += ' ' + segments[i]['text']\n",
    "                current_segment['words'].extend(segments[i]['words'])\n",
    "            else:\n",
    "                optimized_segments.append(current_segment)\n",
    "                current_segment = segments[i]\n",
    "        \n",
    "        optimized_segments.append(current_segment)\n",
    "        return optimized_segments\n",
    "\n",
    "    def process_video(self, url, output_file='segments.json'):\n",
    "        \"\"\"Main processing pipeline\"\"\"\n",
    "        # Download video and extract audio\n",
    "        audio_path = self.download_video(url)\n",
    "        \n",
    "        # Transcribe audio\n",
    "        transcription = self.transcribe_audio(audio_path)\n",
    "        \n",
    "        # Detect voice activity\n",
    "        voice_activity = self.detect_voice_activity(audio_path)\n",
    "        \n",
    "        # Create segments\n",
    "        segments = self.create_segments(transcription, voice_activity, audio_path)\n",
    "        \n",
    "        # Save results\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(segments, f, indent=2)\n",
    "        \n",
    "        return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shawn\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shawn\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "C:\\Users\\shawn\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:truncated_id] Extracting URL: https://www.youtube.com/watch?v=uleG_Vecis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube:truncated_id] uleG_Vecis: Incomplete YouTube ID uleG_Vecis. URL https://www.youtube.com/watch?v=uleG_Vecis looks truncated.\n"
     ]
    },
    {
     "ename": "DownloadError",
     "evalue": "ERROR: [youtube:truncated_id] uleG_Vecis: Incomplete YouTube ID uleG_Vecis. URL https://www.youtube.com/watch?v=uleG_Vecis looks truncated.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExtractorError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\yt_dlp\\YoutubeDL.py:1637\u001b[0m, in \u001b[0;36mYoutubeDL._handle_extraction_exceptions.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1636\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (CookieLoadError, DownloadCancelled, LazyList\u001b[38;5;241m.\u001b[39mIndexError, PagedList\u001b[38;5;241m.\u001b[39mIndexError):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\yt_dlp\\YoutubeDL.py:1772\u001b[0m, in \u001b[0;36mYoutubeDL.__extract_info\u001b[1;34m(self, url, ie, download, extra_info, process)\u001b[0m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1772\u001b[0m     ie_result \u001b[38;5;241m=\u001b[39m \u001b[43mie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UserNotLive \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\yt_dlp\\extractor\\common.py:742\u001b[0m, in \u001b[0;36mInfoExtractor.extract\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_screen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracting URL: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    741\u001b[0m     url \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m truncate_string(url, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m20\u001b[39m)))\n\u001b[1;32m--> 742\u001b[0m ie_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_real_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ie_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\yt_dlp\\extractor\\youtube.py:7986\u001b[0m, in \u001b[0;36mYoutubeTruncatedIDIE._real_extract\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m   7985\u001b[0m video_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_id(url)\n\u001b[1;32m-> 7986\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ExtractorError(\n\u001b[0;32m   7987\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIncomplete YouTube ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m looks truncated.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   7988\u001b[0m     expected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mExtractorError\u001b[0m: [youtube:truncated_id] uleG_Vecis: Incomplete YouTube ID uleG_Vecis. URL https://www.youtube.com/watch?v=uleG_Vecis looks truncated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDownloadError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m segmenter \u001b[38;5;241m=\u001b[39m VideoSegmenter(max_segment_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43msegmenter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.youtube.com/watch?v=uleG_Vecis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 151\u001b[0m, in \u001b[0;36mVideoSegmenter.process_video\u001b[1;34m(self, url, output_file)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Main processing pipeline\"\"\"\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Download video and extract audio\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Transcribe audio\u001b[39;00m\n\u001b[0;32m    154\u001b[0m transcription \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscribe_audio(audio_path)\n",
      "Cell \u001b[1;32mIn[2], line 37\u001b[0m, in \u001b[0;36mVideoSegmenter.download_video\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     26\u001b[0m ydl_opts \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbestaudio/best\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostprocessors\u001b[39m\u001b[38;5;124m'\u001b[39m: [{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouttmpl\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio.\u001b[39m\u001b[38;5;132;01m%(ext)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     34\u001b[0m }\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m yt_dlp\u001b[38;5;241m.\u001b[39mYoutubeDL(ydl_opts) \u001b[38;5;28;01mas\u001b[39;00m ydl:\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mydl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43murl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\yt_dlp\\YoutubeDL.py:3618\u001b[0m, in \u001b[0;36mYoutubeDL.download\u001b[1;34m(self, url_list)\u001b[0m\n\u001b[0;32m   3615\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SameFileError(outtmpl)\n\u001b[0;32m   3617\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m url_list:\n\u001b[1;32m-> 3618\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__download_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3619\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_generic_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforce_generic_extractor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3621\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_retcode\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\yt_dlp\\YoutubeDL.py:3591\u001b[0m, in \u001b[0;36mYoutubeDL.__download_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   3589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3590\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3591\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3592\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m CookieLoadError:\n\u001b[0;32m   3593\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\yt_dlp\\YoutubeDL.py:1626\u001b[0m, in \u001b[0;36mYoutubeDL.extract_info\u001b[1;34m(self, url, download, ie_key, extra_info, process, force_generic_extractor)\u001b[0m\n\u001b[0;32m   1624\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ExistingVideoReached\n\u001b[0;32m   1625\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__extract_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_info_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1628\u001b[0m     extractors_restricted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallowed_extractors\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\yt_dlp\\YoutubeDL.py:1655\u001b[0m, in \u001b[0;36mYoutubeDL._handle_extraction_exceptions.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport_error(msg)\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ExtractorError \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# An error we somewhat expected\u001b[39;00m\n\u001b[1;32m-> 1655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreport_error\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignoreerrors\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\yt_dlp\\YoutubeDL.py:1095\u001b[0m, in \u001b[0;36mYoutubeDL.report_error\u001b[1;34m(self, message, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreport_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, message, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m    Do the same as trouble, but prefixes the message with 'ERROR:', colored\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;124;03m    in red if stderr is a tty file.\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1095\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrouble\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_err\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mERROR:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStyles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mERROR\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmessage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\yt_dlp\\YoutubeDL.py:1034\u001b[0m, in \u001b[0;36mYoutubeDL.trouble\u001b[1;34m(self, message, tb, is_error)\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m         exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[1;32m-> 1034\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DownloadError(message, exc_info)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_retcode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mDownloadError\u001b[0m: ERROR: [youtube:truncated_id] uleG_Vecis: Incomplete YouTube ID uleG_Vecis. URL https://www.youtube.com/watch?v=uleG_Vecis looks truncated."
     ]
    }
   ],
   "source": [
    "segmenter = VideoSegmenter(max_segment_length=15)\n",
    "segments = segmenter.process_video(\"https://www.youtube.com/watch?v=uleG_Vecis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created output directory: segments\n",
      "INFO:__main__:Starting audio segmentation process for 100+ Computer Science Concepts Explained.wav\n",
      "INFO:__main__:Loaded audio file: duration=787.30s\n",
      "INFO:__main__:Parsing VTT file: -uleG_Vecis.vtt\n",
      "INFO:__main__:Successfully parsed 417 segments from VTT\n",
      "INFO:__main__:Optimizing segments with max duration of 15.0 seconds\n",
      "INFO:__main__:Segment optimization complete. Created 65 segments\n",
      "INFO:__main__:Saved segment 0: segments\\segment_000_0000080_0013359.wav\n",
      "INFO:__main__:Saved segment 1: segments\\segment_001_0011279_0024719.wav\n",
      "INFO:__main__:Saved segment 2: segments\\segment_002_0023519_0037679.wav\n",
      "INFO:__main__:Saved segment 3: segments\\segment_003_0035759_0050399.wav\n",
      "INFO:__main__:Saved segment 4: segments\\segment_004_0048479_0063280.wav\n",
      "INFO:__main__:Saved segment 5: segments\\segment_005_0060878_0074479.wav\n",
      "INFO:__main__:Saved segment 6: segments\\segment_006_0072799_0087200.wav\n",
      "INFO:__main__:Saved segment 7: segments\\segment_007_0085040_0099280.wav\n",
      "INFO:__main__:Saved segment 8: segments\\segment_008_0097759_0110560.wav\n",
      "INFO:__main__:Saved segment 9: segments\\segment_009_0108640_0123359.wav\n",
      "INFO:__main__:Saved segment 10: segments\\segment_010_0121680_0135199.wav\n",
      "INFO:__main__:Saved segment 11: segments\\segment_011_0133520_0148400.wav\n",
      "INFO:__main__:Saved segment 12: segments\\segment_012_0146318_0159839.wav\n",
      "INFO:__main__:Saved segment 13: segments\\segment_013_0157919_0172079.wav\n",
      "INFO:__main__:Saved segment 14: segments\\segment_014_0170080_0184560.wav\n",
      "INFO:__main__:Saved segment 15: segments\\segment_015_0182239_0196560.wav\n",
      "INFO:__main__:Saved segment 16: segments\\segment_016_0194878_0209199.wav\n",
      "INFO:__main__:Saved segment 17: segments\\segment_017_0206878_0220799.wav\n",
      "INFO:__main__:Saved segment 18: segments\\segment_018_0218799_0232079.wav\n",
      "INFO:__main__:Saved segment 19: segments\\segment_019_0230639_0244238.wav\n",
      "INFO:__main__:Saved segment 20: segments\\segment_020_0242158_0255839.wav\n",
      "INFO:__main__:Saved segment 21: segments\\segment_021_0256060_0270319.wav\n",
      "INFO:__main__:Saved segment 22: segments\\segment_022_0268639_0283120.wav\n",
      "INFO:__main__:Saved segment 23: segments\\segment_023_0281360_0295600.wav\n",
      "INFO:__main__:Saved segment 24: segments\\segment_024_0293279_0307600.wav\n",
      "INFO:__main__:Saved segment 25: segments\\segment_025_0305279_0319918.wav\n",
      "INFO:__main__:Saved segment 26: segments\\segment_026_0318000_0331519.wav\n",
      "INFO:__main__:Saved segment 27: segments\\segment_027_0329439_0343439.wav\n",
      "INFO:__main__:Saved segment 28: segments\\segment_028_0341439_0354720.wav\n",
      "INFO:__main__:Saved segment 29: segments\\segment_029_0353038_0366160.wav\n",
      "INFO:__main__:Saved segment 30: segments\\segment_030_0364639_0378639.wav\n",
      "INFO:__main__:Saved segment 31: segments\\segment_031_0376800_0391599.wav\n",
      "INFO:__main__:Saved segment 32: segments\\segment_032_0389600_0404079.wav\n",
      "INFO:__main__:Saved segment 33: segments\\segment_033_0402319_0416159.wav\n",
      "INFO:__main__:Saved segment 34: segments\\segment_034_0414319_0427680.wav\n",
      "INFO:__main__:Saved segment 35: segments\\segment_035_0425199_0439839.wav\n",
      "INFO:__main__:Saved segment 36: segments\\segment_036_0438000_0451360.wav\n",
      "INFO:__main__:Saved segment 37: segments\\segment_037_0449519_0463519.wav\n",
      "INFO:__main__:Saved segment 38: segments\\segment_038_0461759_0476639.wav\n",
      "INFO:__main__:Saved segment 39: segments\\segment_039_0474800_0489199.wav\n",
      "INFO:__main__:Saved segment 40: segments\\segment_040_0487439_0501759.wav\n",
      "INFO:__main__:Saved segment 41: segments\\segment_041_0499839_0513438.wav\n",
      "INFO:__main__:Saved segment 42: segments\\segment_042_0512000_0526240.wav\n",
      "INFO:__main__:Saved segment 43: segments\\segment_043_0524080_0538639.wav\n",
      "INFO:__main__:Saved segment 44: segments\\segment_044_0536879_0551679.wav\n",
      "INFO:__main__:Saved segment 45: segments\\segment_045_0549839_0564639.wav\n",
      "INFO:__main__:Saved segment 46: segments\\segment_046_0563278_0577360.wav\n",
      "INFO:__main__:Saved segment 47: segments\\segment_047_0575440_0590319.wav\n",
      "INFO:__main__:Saved segment 48: segments\\segment_048_0588399_0602879.wav\n",
      "INFO:__main__:Saved segment 49: segments\\segment_049_0600879_0614639.wav\n",
      "INFO:__main__:Saved segment 50: segments\\segment_050_0613039_0626240.wav\n",
      "INFO:__main__:Saved segment 51: segments\\segment_051_0624399_0637919.wav\n",
      "INFO:__main__:Saved segment 52: segments\\segment_052_0635519_0649519.wav\n",
      "INFO:__main__:Saved segment 53: segments\\segment_053_0647519_0661839.wav\n",
      "INFO:__main__:Saved segment 54: segments\\segment_054_0660078_0674240.wav\n",
      "INFO:__main__:Saved segment 55: segments\\segment_055_0672958_0686879.wav\n",
      "INFO:__main__:Saved segment 56: segments\\segment_056_0684958_0698159.wav\n",
      "INFO:__main__:Saved segment 57: segments\\segment_057_0696799_0711519.wav\n",
      "INFO:__main__:Saved segment 58: segments\\segment_058_0709600_0722800.wav\n",
      "INFO:__main__:Saved segment 59: segments\\segment_059_0721278_0734720.wav\n",
      "INFO:__main__:Saved segment 60: segments\\segment_060_0732240_0746958.wav\n",
      "INFO:__main__:Saved segment 61: segments\\segment_061_0744799_0758959.wav\n",
      "INFO:__main__:Saved segment 62: segments\\segment_062_0757200_0771360.wav\n",
      "INFO:__main__:Saved segment 63: segments\\segment_063_0769600_0783120.wav\n",
      "INFO:__main__:Saved segment 64: segments\\segment_064_0781519_0789120.wav\n",
      "INFO:__main__:Processing complete. Created 65 segments\n",
      "INFO:__main__:Segmentation completed successfully\n",
      "INFO:__main__:Created segment pair: segments\\segment_000_0000080_0013359.wav - segments\\segment_000.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_001_0011279_0024719.wav - segments\\segment_001.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_002_0023519_0037679.wav - segments\\segment_002.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_003_0035759_0050399.wav - segments\\segment_003.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_004_0048479_0063280.wav - segments\\segment_004.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_005_0060878_0074479.wav - segments\\segment_005.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_006_0072799_0087200.wav - segments\\segment_006.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_007_0085040_0099280.wav - segments\\segment_007.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_008_0097759_0110560.wav - segments\\segment_008.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_009_0108640_0123359.wav - segments\\segment_009.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_010_0121680_0135199.wav - segments\\segment_010.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_011_0133520_0148400.wav - segments\\segment_011.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_012_0146318_0159839.wav - segments\\segment_012.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_013_0157919_0172079.wav - segments\\segment_013.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_014_0170080_0184560.wav - segments\\segment_014.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_015_0182239_0196560.wav - segments\\segment_015.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_016_0194878_0209199.wav - segments\\segment_016.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_017_0206878_0220799.wav - segments\\segment_017.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_018_0218799_0232079.wav - segments\\segment_018.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_019_0230639_0244238.wav - segments\\segment_019.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_020_0242158_0255839.wav - segments\\segment_020.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_021_0256060_0270319.wav - segments\\segment_021.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_022_0268639_0283120.wav - segments\\segment_022.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_023_0281360_0295600.wav - segments\\segment_023.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_024_0293279_0307600.wav - segments\\segment_024.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_025_0305279_0319918.wav - segments\\segment_025.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_026_0318000_0331519.wav - segments\\segment_026.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_027_0329439_0343439.wav - segments\\segment_027.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_028_0341439_0354720.wav - segments\\segment_028.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_029_0353038_0366160.wav - segments\\segment_029.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_030_0364639_0378639.wav - segments\\segment_030.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_031_0376800_0391599.wav - segments\\segment_031.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_032_0389600_0404079.wav - segments\\segment_032.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_033_0402319_0416159.wav - segments\\segment_033.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_034_0414319_0427680.wav - segments\\segment_034.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_035_0425199_0439839.wav - segments\\segment_035.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_036_0438000_0451360.wav - segments\\segment_036.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_037_0449519_0463519.wav - segments\\segment_037.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_038_0461759_0476639.wav - segments\\segment_038.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_039_0474800_0489199.wav - segments\\segment_039.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_040_0487439_0501759.wav - segments\\segment_040.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_041_0499839_0513438.wav - segments\\segment_041.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_042_0512000_0526240.wav - segments\\segment_042.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_043_0524080_0538639.wav - segments\\segment_043.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_044_0536879_0551679.wav - segments\\segment_044.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_045_0549839_0564639.wav - segments\\segment_045.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_046_0563278_0577360.wav - segments\\segment_046.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_047_0575440_0590319.wav - segments\\segment_047.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_048_0588399_0602879.wav - segments\\segment_048.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_049_0600879_0614639.wav - segments\\segment_049.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_050_0613039_0626240.wav - segments\\segment_050.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_051_0624399_0637919.wav - segments\\segment_051.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_052_0635519_0649519.wav - segments\\segment_052.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_053_0647519_0661839.wav - segments\\segment_053.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_054_0660078_0674240.wav - segments\\segment_054.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_055_0672958_0686879.wav - segments\\segment_055.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_056_0684958_0698159.wav - segments\\segment_056.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_057_0696799_0711519.wav - segments\\segment_057.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_058_0709600_0722800.wav - segments\\segment_058.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_059_0721278_0734720.wav - segments\\segment_059.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_060_0732240_0746958.wav - segments\\segment_060.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_061_0744799_0758959.wav - segments\\segment_061.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_062_0757200_0771360.wav - segments\\segment_062.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_063_0769600_0783120.wav - segments\\segment_063.txt\n",
      "INFO:__main__:Created segment pair: segments\\segment_064_0781519_0789120.wav - segments\\segment_064.txt\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pydub import AudioSegment\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "import wave\n",
    "import contextlib\n",
    "import datetime\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('audio_segmentation.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AudioSegmenter:\n",
    "    def __init__(self, audio_path: str, vtt_path: str, output_dir: str = \"segments\"):\n",
    "        \"\"\"\n",
    "        Initialize the audio segmenter\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to WAV audio file\n",
    "            vtt_path: Path to VTT transcript file\n",
    "            output_dir: Directory to save segmented audio files\n",
    "        \"\"\"\n",
    "        self.audio_path = audio_path\n",
    "        self.vtt_path = vtt_path\n",
    "        self.output_dir = output_dir\n",
    "        self.vad = webrtcvad.Vad(3)  # Aggressiveness level 3\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            logger.info(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "    def parse_vtt(self) -> List[Dict]:\n",
    "        \"\"\"Parse VTT file and extract timestamps and text\"\"\"\n",
    "        logger.info(f\"Parsing VTT file: {self.vtt_path}\")\n",
    "        segments = []\n",
    "        \n",
    "        try:\n",
    "            with open(self.vtt_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Split into segments\n",
    "            blocks = content.split('\\n\\n')\n",
    "            timestamp_pattern = r'(\\d{2}:\\d{2}:\\d{2}\\.\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}\\.\\d{3})'\n",
    "            \n",
    "            for block in blocks:\n",
    "                if not block.strip() or block.strip() == 'WEBVTT':\n",
    "                    continue\n",
    "                    \n",
    "                lines = block.strip().split('\\n')\n",
    "                timestamp_match = re.search(timestamp_pattern, lines[0])\n",
    "                \n",
    "                if timestamp_match:\n",
    "                    start_time = self._time_to_seconds(timestamp_match.group(1))\n",
    "                    end_time = self._time_to_seconds(timestamp_match.group(2))\n",
    "                    text = ' '.join(lines[1:])\n",
    "                    \n",
    "                    segments.append({\n",
    "                        'start': start_time,\n",
    "                        'end': end_time,\n",
    "                        'text': text\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Successfully parsed {len(segments)} segments from VTT\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing VTT file: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _time_to_seconds(self, time_str: str) -> float:\n",
    "        \"\"\"Convert timestamp string to seconds\"\"\"\n",
    "        time_obj = datetime.datetime.strptime(time_str, '%H:%M:%S.%f')\n",
    "        return time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second + time_obj.microsecond/1000000\n",
    "\n",
    "    def detect_voice_activity(self, audio_segment: AudioSegment, window_size: int = 30) -> List[bool]:\n",
    "        \"\"\"\n",
    "        Detect voice activity in audio segment\n",
    "        \n",
    "        Args:\n",
    "            audio_segment: Audio segment to analyze\n",
    "            window_size: Size of window in milliseconds\n",
    "            \n",
    "        Returns:\n",
    "            List of booleans indicating voice activity\n",
    "        \"\"\"\n",
    "        logger.info(\"Detecting voice activity\")\n",
    "        try:\n",
    "            # Convert audio to format compatible with WebRTC VAD\n",
    "            audio_segment = audio_segment.set_channels(1)  # Convert to mono\n",
    "            audio_segment = audio_segment.set_frame_rate(16000)  # Convert to 16kHz\n",
    "            \n",
    "            samples = np.array(audio_segment.get_array_of_samples())\n",
    "            samples_per_window = int(window_size * 16000 / 1000)  # 16kHz sample rate\n",
    "            \n",
    "            voice_activity = []\n",
    "            for start in range(0, len(samples), samples_per_window):\n",
    "                chunk = samples[start:start + samples_per_window]\n",
    "                if len(chunk) == samples_per_window:  # Only process full chunks\n",
    "                    is_speech = self.vad.is_speech(chunk.tobytes(), sample_rate=16000)\n",
    "                    voice_activity.append(is_speech)\n",
    "            \n",
    "            logger.info(f\"Voice activity detection completed. Found {sum(voice_activity)} active windows\")\n",
    "            return voice_activity\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in voice activity detection: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def optimize_segments(self, segments: List[Dict], max_duration: float = 15.0) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Optimize segments based on maximum duration and voice activity\n",
    "        \n",
    "        Args:\n",
    "            segments: List of segments with start time, end time, and text\n",
    "            max_duration: Maximum duration of each segment in seconds\n",
    "            \n",
    "        Returns:\n",
    "            List of optimized segments\n",
    "        \"\"\"\n",
    "        logger.info(f\"Optimizing segments with max duration of {max_duration} seconds\")\n",
    "        optimized_segments = []\n",
    "        current_segment = None\n",
    "        \n",
    "        try:\n",
    "            for segment in segments:\n",
    "                if current_segment is None:\n",
    "                    current_segment = segment.copy()\n",
    "                    continue\n",
    "                \n",
    "                merged_duration = segment['end'] - current_segment['start']\n",
    "                \n",
    "                if merged_duration > max_duration:\n",
    "                    optimized_segments.append(current_segment)\n",
    "                    current_segment = segment.copy()\n",
    "                else:\n",
    "                    current_segment['end'] = segment['end']\n",
    "                    current_segment['text'] += ' ' + segment['text']\n",
    "            \n",
    "            if current_segment:\n",
    "                optimized_segments.append(current_segment)\n",
    "                \n",
    "            logger.info(f\"Segment optimization complete. Created {len(optimized_segments)} segments\")\n",
    "            return optimized_segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in segment optimization: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_segment(self, audio: AudioSegment, segment: Dict, index: int):\n",
    "        \"\"\"Save audio segment to file\"\"\"\n",
    "        try:\n",
    "            start_ms = int(segment['start'] * 1000)\n",
    "            end_ms = int(segment['end'] * 1000)\n",
    "            segment_audio = audio[start_ms:end_ms]\n",
    "            \n",
    "            # Create filename with timestamp information\n",
    "            filename = f\"segment_{index:03d}_{start_ms:07d}_{end_ms:07d}.wav\"\n",
    "            filepath = os.path.join(self.output_dir, filename)\n",
    "            \n",
    "            segment_audio.export(filepath, format=\"wav\")\n",
    "            logger.info(f\"Saved segment {index}: {filepath}\")\n",
    "            \n",
    "            # Save corresponding text\n",
    "            text_filepath = os.path.join(self.output_dir, f\"segment_{index:03d}.txt\")\n",
    "            with open(text_filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(segment['text'])\n",
    "            \n",
    "            return filepath\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving segment {index}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process(self, max_duration: float = 15.0) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Process audio file and create segments\n",
    "        \n",
    "        Args:\n",
    "            max_duration: Maximum duration of each segment in seconds\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples containing (audio_path, text_path) for each segment\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting audio segmentation process for {self.audio_path}\")\n",
    "        try:\n",
    "            # Load audio file\n",
    "            audio = AudioSegment.from_wav(self.audio_path)\n",
    "            logger.info(f\"Loaded audio file: duration={len(audio)/1000:.2f}s\")\n",
    "            \n",
    "            # Parse VTT file\n",
    "            segments = self.parse_vtt()\n",
    "            \n",
    "            # Optimize segments\n",
    "            optimized_segments = self.optimize_segments(segments, max_duration)\n",
    "            \n",
    "            # Save segments\n",
    "            output_files = []\n",
    "            for i, segment in enumerate(optimized_segments):\n",
    "                audio_path = self.save_segment(audio, segment, i)\n",
    "                text_path = os.path.join(self.output_dir, f\"segment_{i:03d}.txt\")\n",
    "                output_files.append((audio_path, text_path))\n",
    "            \n",
    "            logger.info(f\"Processing complete. Created {len(output_files)} segments\")\n",
    "            return output_files\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in main processing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    segmenter = AudioSegmenter(\n",
    "        audio_path=\"100+ Computer Science Concepts Explained.wav\",\n",
    "        vtt_path=\"-uleG_Vecis.vtt\",\n",
    "        output_dir=\"segments\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        output_files = segmenter.process(max_duration=15.0)\n",
    "        logger.info(\"Segmentation completed successfully\")\n",
    "        for audio_path, text_path in output_files:\n",
    "            logger.info(f\"Created segment pair: {audio_path} - {text_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Segmentation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.4.1+cpu available.\n",
      "INFO:datasets:TensorFlow version 2.18.0 available.\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "C:\\Users\\shawn\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "ERROR:__main__:Error creating/opening table: Schema must be an instance of pyarrow.Schema\n",
      "ERROR:__main__:Error indexing segments: Schema must be an instance of pyarrow.Schema\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Schema must be an instance of pyarrow.Schema\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "import torch\n",
    "\n",
    "class SegmentVectorStore:\n",
    "    def __init__(self, segments_dir: str, db_path: str = \"segments_db\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store for audio segments\n",
    "        \n",
    "        Args:\n",
    "            segments_dir: Directory containing audio segments and transcripts\n",
    "            db_path: Path to store LanceDB database\n",
    "        \"\"\"\n",
    "        self.segments_dir = segments_dir\n",
    "        self.db_path = db_path\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Initialize sentence transformer for embeddings\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "        \n",
    "        # Initialize LanceDB\n",
    "        self.db = lancedb.connect(db_path)\n",
    "        \n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def create_table(self, table_name: str = \"segments\") -> None:\n",
    "        \"\"\"Create or get LanceDB table\"\"\"\n",
    "        try:\n",
    "            # Define schema for the table\n",
    "            schema = {\n",
    "                \"id\": \"string\",\n",
    "                \"text\": \"string\",\n",
    "                \"audio_path\": \"string\",\n",
    "                \"start_time\": \"float\",\n",
    "                \"end_time\": \"float\",\n",
    "                \"vector\": \"vector(384)\"  # dimension of all-MiniLM-L6-v2 embeddings\n",
    "            }\n",
    "            \n",
    "            # Create table if it doesn't exist\n",
    "            if table_name not in self.db.table_names():\n",
    "                self.table = self.db.create_table(table_name, schema=schema)\n",
    "                self.logger.info(f\"Created new table: {table_name}\")\n",
    "            else:\n",
    "                self.table = self.db.open_table(table_name)\n",
    "                self.logger.info(f\"Opened existing table: {table_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating/opening table: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_segments(self) -> List[Dict]:\n",
    "        \"\"\"Load all segments from the segments directory\"\"\"\n",
    "        segments = []\n",
    "        try:\n",
    "            # Get all segment files\n",
    "            files = os.listdir(self.segments_dir)\n",
    "            audio_files = [f for f in files if f.endswith('.wav')]\n",
    "            \n",
    "            for audio_file in audio_files:\n",
    "                # Get corresponding text file\n",
    "                base_name = audio_file.rsplit('.', 1)[0]\n",
    "                text_file = f\"{base_name}.txt\"\n",
    "                \n",
    "                if text_file in files:\n",
    "                    # Parse timestamp information from filename\n",
    "                    # Format: segment_XXX_SSSSSSS_EEEEEEE.wav\n",
    "                    parts = base_name.split('_')\n",
    "                    start_time = float(parts[2]) / 1000  # Convert ms to seconds\n",
    "                    end_time = float(parts[3]) / 1000\n",
    "                    \n",
    "                    # Read text content\n",
    "                    with open(os.path.join(self.segments_dir, text_file), 'r', encoding='utf-8') as f:\n",
    "                        text = f.read().strip()\n",
    "                    \n",
    "                    segments.append({\n",
    "                        'id': base_name,\n",
    "                        'text': text,\n",
    "                        'audio_path': os.path.join(self.segments_dir, audio_file),\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time\n",
    "                    })\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, segments: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Create embeddings for segments\"\"\"\n",
    "        try:\n",
    "            # Get text from segments\n",
    "            texts = [seg['text'] for seg in segments]\n",
    "            \n",
    "            # Create embeddings\n",
    "            embeddings = self.model.encode(texts)\n",
    "            \n",
    "            # Add embeddings to segments\n",
    "            for seg, emb in zip(segments, embeddings):\n",
    "                seg['vector'] = emb\n",
    "            \n",
    "            self.logger.info(f\"Created embeddings for {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def index_segments(self, table_name: str = \"segments\") -> None:\n",
    "        \"\"\"Index all segments in the vector store\"\"\"\n",
    "        try:\n",
    "            # Create/open table\n",
    "            self.create_table(table_name)\n",
    "            \n",
    "            # Load segments\n",
    "            segments = self.load_segments()\n",
    "            \n",
    "            # Create embeddings\n",
    "            segments_with_embeddings = self.create_embeddings(segments)\n",
    "            \n",
    "            # Add to LanceDB\n",
    "            self.table.add(segments_with_embeddings)\n",
    "            \n",
    "            self.logger.info(f\"Indexed {len(segments)} segments in LanceDB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error indexing segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def search(self, query: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for similar segments using a text query\n",
    "        \n",
    "        Args:\n",
    "            query: Text query to search for\n",
    "            limit: Maximum number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of most similar segments with their metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create query embedding\n",
    "            query_embedding = self.model.encode(query)\n",
    "            \n",
    "            # Search in LanceDB\n",
    "            results = self.table.search(query_embedding)\\\n",
    "                              .limit(limit)\\\n",
    "                              .select([\"id\", \"text\", \"audio_path\", \"start_time\", \"end_time\", \"_distance\"])\\\n",
    "                              .to_list()\n",
    "            \n",
    "            self.logger.info(f\"Found {len(results)} results for query: {query}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error searching segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_range_search(self, query: str, distance_threshold: float = 0.3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for segments within a semantic distance threshold\n",
    "        \n",
    "        Args:\n",
    "            query: Text query to search for\n",
    "            distance_threshold: Maximum semantic distance to include in results\n",
    "            \n",
    "        Returns:\n",
    "            List of segments within the distance threshold\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_embedding = self.model.encode(query)\n",
    "            \n",
    "            results = self.table.search(query_embedding)\\\n",
    "                              .where(f\"_distance < {distance_threshold}\")\\\n",
    "                              .select([\"id\", \"text\", \"audio_path\", \"start_time\", \"end_time\", \"_distance\"])\\\n",
    "                              .to_list()\n",
    "            \n",
    "            self.logger.info(f\"Found {len(results)} results within distance {distance_threshold} for query: {query}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in semantic range search: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize vector store\n",
    "    vector_store = SegmentVectorStore(\n",
    "        segments_dir=\"segments\",\n",
    "        db_path=\"segments_db\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Index segments\n",
    "        vector_store.index_segments()\n",
    "        \n",
    "        # Example search\n",
    "        query = \"turning machine\"\n",
    "        results = vector_store.search(query, limit=5)\n",
    "        \n",
    "        print(\"\\nSearch Results:\")\n",
    "        for result in results:\n",
    "            print(f\"\\nSegment ID: {result['id']}\")\n",
    "            print(f\"Text: {result['text']}\")\n",
    "            print(f\"Audio Path: {result['audio_path']}\")\n",
    "            print(f\"Time Range: {result['start_time']:.2f}s - {result['end_time']:.2f}s\")\n",
    "            print(f\"Distance: {result['_distance']:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Created new table: segments\n",
      "INFO:__main__:Loaded 0 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695c9ac97a3a48a6af5e1545572067b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created embeddings for 0 segments\n",
      "ERROR:__main__:Error indexing segments: 'Field \"id\" does not exist in schema'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'Field \"id\" does not exist in schema'\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "\n",
    "class SegmentVectorStore:\n",
    "    def __init__(self, segments_dir: str, db_path: str = \"segments_db\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store for audio segments\n",
    "        \n",
    "        Args:\n",
    "            segments_dir: Directory containing audio segments and transcripts\n",
    "            db_path: Path to store LanceDB database\n",
    "        \"\"\"\n",
    "        self.segments_dir = segments_dir\n",
    "        self.db_path = db_path\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Initialize sentence transformer for embeddings\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "        \n",
    "        # Initialize LanceDB\n",
    "        self.db = lancedb.connect(db_path)\n",
    "        \n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def create_table(self, table_name: str = \"segments\") -> None:\n",
    "        \"\"\"Create or get LanceDB table\"\"\"\n",
    "        try:\n",
    "            # Define schema using PyArrow\n",
    "            schema = pa.schema([\n",
    "                pa.field('id', pa.string()),\n",
    "                pa.field('text', pa.string()),\n",
    "                pa.field('audio_path', pa.string()),\n",
    "                pa.field('start_time', pa.float32()),\n",
    "                pa.field('end_time', pa.float32()),\n",
    "                pa.field('vector', pa.list_(pa.float32(), 384))  # dimension of all-MiniLM-L6-v2 embeddings\n",
    "            ])\n",
    "            \n",
    "            # Create table if it doesn't exist\n",
    "            if table_name not in self.db.table_names():\n",
    "                self.table = self.db.create_table(table_name, schema=schema)\n",
    "                self.logger.info(f\"Created new table: {table_name}\")\n",
    "            else:\n",
    "                self.table = self.db.open_table(table_name)\n",
    "                self.logger.info(f\"Opened existing table: {table_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating/opening table: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_segments(self) -> List[Dict]:\n",
    "        \"\"\"Load all segments from the segments directory\"\"\"\n",
    "        segments = []\n",
    "        try:\n",
    "            # Get all segment files\n",
    "            files = os.listdir(self.segments_dir)\n",
    "            audio_files = [f for f in files if f.endswith('.wav')]\n",
    "            \n",
    "            for audio_file in audio_files:\n",
    "                # Get corresponding text file\n",
    "                base_name = audio_file.rsplit('.', 1)[0]\n",
    "                text_file = f\"{base_name}.txt\"\n",
    "                \n",
    "                if text_file in files:\n",
    "                    # Parse timestamp information from filename\n",
    "                    # Format: segment_XXX_SSSSSSS_EEEEEEE.wav\n",
    "                    parts = base_name.split('_')\n",
    "                    start_time = float(parts[2]) / 1000  # Convert ms to seconds\n",
    "                    end_time = float(parts[3]) / 1000\n",
    "                    \n",
    "                    # Read text content\n",
    "                    with open(os.path.join(self.segments_dir, text_file), 'r', encoding='utf-8') as f:\n",
    "                        text = f.read().strip()\n",
    "                    \n",
    "                    segments.append({\n",
    "                        'id': base_name,\n",
    "                        'text': text,\n",
    "                        'audio_path': os.path.join(self.segments_dir, audio_file),\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time\n",
    "                    })\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, segments: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Create embeddings for segments\"\"\"\n",
    "        try:\n",
    "            # Get text from segments\n",
    "            texts = [seg['text'] for seg in segments]\n",
    "            \n",
    "            # Create embeddings\n",
    "            embeddings = self.model.encode(texts)\n",
    "            \n",
    "            # Convert embeddings to list format for PyArrow compatibility\n",
    "            for seg, emb in zip(segments, embeddings):\n",
    "                seg['vector'] = emb.tolist()  # Convert numpy array to list\n",
    "            \n",
    "            self.logger.info(f\"Created embeddings for {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def index_segments(self, table_name: str = \"segments\") -> None:\n",
    "        \"\"\"Index all segments in the vector store\"\"\"\n",
    "        try:\n",
    "            # Create/open table\n",
    "            self.create_table(table_name)\n",
    "            \n",
    "            # Load segments\n",
    "            segments = self.load_segments()\n",
    "            \n",
    "            # Create embeddings\n",
    "            segments_with_embeddings = self.create_embeddings(segments)\n",
    "            \n",
    "            # Convert to PyArrow table and add to LanceDB\n",
    "            data = pa.Table.from_pylist(segments_with_embeddings)\n",
    "            self.table.add(data)\n",
    "            \n",
    "            self.logger.info(f\"Indexed {len(segments)} segments in LanceDB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error indexing segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def search(self, query: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for similar segments using a text query\n",
    "        \n",
    "        Args:\n",
    "            query: Text query to search for\n",
    "            limit: Maximum number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of most similar segments with their metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create query embedding\n",
    "            query_embedding = self.model.encode(query).tolist()  # Convert to list for compatibility\n",
    "            \n",
    "            # Search in LanceDB\n",
    "            results = self.table.search(query_embedding)\\\n",
    "                              .limit(limit)\\\n",
    "                              .select([\"id\", \"text\", \"audio_path\", \"start_time\", \"end_time\", \"_distance\"])\\\n",
    "                              .to_list()\n",
    "            \n",
    "            self.logger.info(f\"Found {len(results)} results for query: {query}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error searching segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_range_search(self, query: str, distance_threshold: float = 0.3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for segments within a semantic distance threshold\n",
    "        \n",
    "        Args:\n",
    "            query: Text query to search for\n",
    "            distance_threshold: Maximum semantic distance to include in results\n",
    "            \n",
    "        Returns:\n",
    "            List of segments within the distance threshold\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_embedding = self.model.encode(query).tolist()  # Convert to list for compatibility\n",
    "            \n",
    "            results = self.table.search(query_embedding)\\\n",
    "                              .where(f\"_distance < {distance_threshold}\")\\\n",
    "                              .select([\"id\", \"text\", \"audio_path\", \"start_time\", \"end_time\", \"_distance\"])\\\n",
    "                              .to_list()\n",
    "            \n",
    "            self.logger.info(f\"Found {len(results)} results within distance {distance_threshold} for query: {query}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in semantic range search: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize vector store\n",
    "    vector_store = SegmentVectorStore(\n",
    "        segments_dir=\"segments\",\n",
    "        db_path=\"segments_db\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Index segments\n",
    "        vector_store.index_segments()\n",
    "        \n",
    "        # Example search\n",
    "        query = \"turing machine\"\n",
    "        results = vector_store.search(query, limit=5)\n",
    "        \n",
    "        print(\"\\nSearch Results:\")\n",
    "        for result in results:\n",
    "            print(f\"\\nSegment ID: {result['id']}\")\n",
    "            print(f\"Text: {result['text']}\")\n",
    "            print(f\"Audio Path: {result['audio_path']}\")\n",
    "            print(f\"Time Range: {result['start_time']:.2f}s - {result['end_time']:.2f}s\")\n",
    "            print(f\"Distance: {result['_distance']:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for segments in: c:\\Users\\shawn\\OneDrive\\Desktop\\newbie\\segments\n",
      "Contents of segments directory:\n",
      "  - segment_000.txt\n",
      "  - segment_000_0000080_0013359.wav\n",
      "  - segment_001.txt\n",
      "  - segment_001_0011279_0024719.wav\n",
      "  - segment_002.txt\n",
      "  - segment_002_0023519_0037679.wav\n",
      "  - segment_003.txt\n",
      "  - segment_003_0035759_0050399.wav\n",
      "  - segment_004.txt\n",
      "  - segment_004_0048479_0063280.wav\n",
      "  - segment_005.txt\n",
      "  - segment_005_0060878_0074479.wav\n",
      "  - segment_006.txt\n",
      "  - segment_006_0072799_0087200.wav\n",
      "  - segment_007.txt\n",
      "  - segment_007_0085040_0099280.wav\n",
      "  - segment_008.txt\n",
      "  - segment_008_0097759_0110560.wav\n",
      "  - segment_009.txt\n",
      "  - segment_009_0108640_0123359.wav\n",
      "  - segment_010.txt\n",
      "  - segment_010_0121680_0135199.wav\n",
      "  - segment_011.txt\n",
      "  - segment_011_0133520_0148400.wav\n",
      "  - segment_012.txt\n",
      "  - segment_012_0146318_0159839.wav\n",
      "  - segment_013.txt\n",
      "  - segment_013_0157919_0172079.wav\n",
      "  - segment_014.txt\n",
      "  - segment_014_0170080_0184560.wav\n",
      "  - segment_015.txt\n",
      "  - segment_015_0182239_0196560.wav\n",
      "  - segment_016.txt\n",
      "  - segment_016_0194878_0209199.wav\n",
      "  - segment_017.txt\n",
      "  - segment_017_0206878_0220799.wav\n",
      "  - segment_018.txt\n",
      "  - segment_018_0218799_0232079.wav\n",
      "  - segment_019.txt\n",
      "  - segment_019_0230639_0244238.wav\n",
      "  - segment_020.txt\n",
      "  - segment_020_0242158_0255839.wav\n",
      "  - segment_021.txt\n",
      "  - segment_021_0256060_0270319.wav\n",
      "  - segment_022.txt\n",
      "  - segment_022_0268639_0283120.wav\n",
      "  - segment_023.txt\n",
      "  - segment_023_0281360_0295600.wav\n",
      "  - segment_024.txt\n",
      "  - segment_024_0293279_0307600.wav\n",
      "  - segment_025.txt\n",
      "  - segment_025_0305279_0319918.wav\n",
      "  - segment_026.txt\n",
      "  - segment_026_0318000_0331519.wav\n",
      "  - segment_027.txt\n",
      "  - segment_027_0329439_0343439.wav\n",
      "  - segment_028.txt\n",
      "  - segment_028_0341439_0354720.wav\n",
      "  - segment_029.txt\n",
      "  - segment_029_0353038_0366160.wav\n",
      "  - segment_030.txt\n",
      "  - segment_030_0364639_0378639.wav\n",
      "  - segment_031.txt\n",
      "  - segment_031_0376800_0391599.wav\n",
      "  - segment_032.txt\n",
      "  - segment_032_0389600_0404079.wav\n",
      "  - segment_033.txt\n",
      "  - segment_033_0402319_0416159.wav\n",
      "  - segment_034.txt\n",
      "  - segment_034_0414319_0427680.wav\n",
      "  - segment_035.txt\n",
      "  - segment_035_0425199_0439839.wav\n",
      "  - segment_036.txt\n",
      "  - segment_036_0438000_0451360.wav\n",
      "  - segment_037.txt\n",
      "  - segment_037_0449519_0463519.wav\n",
      "  - segment_038.txt\n",
      "  - segment_038_0461759_0476639.wav\n",
      "  - segment_039.txt\n",
      "  - segment_039_0474800_0489199.wav\n",
      "  - segment_040.txt\n",
      "  - segment_040_0487439_0501759.wav\n",
      "  - segment_041.txt\n",
      "  - segment_041_0499839_0513438.wav\n",
      "  - segment_042.txt\n",
      "  - segment_042_0512000_0526240.wav\n",
      "  - segment_043.txt\n",
      "  - segment_043_0524080_0538639.wav\n",
      "  - segment_044.txt\n",
      "  - segment_044_0536879_0551679.wav\n",
      "  - segment_045.txt\n",
      "  - segment_045_0549839_0564639.wav\n",
      "  - segment_046.txt\n",
      "  - segment_046_0563278_0577360.wav\n",
      "  - segment_047.txt\n",
      "  - segment_047_0575440_0590319.wav\n",
      "  - segment_048.txt\n",
      "  - segment_048_0588399_0602879.wav\n",
      "  - segment_049.txt\n",
      "  - segment_049_0600879_0614639.wav\n",
      "  - segment_050.txt\n",
      "  - segment_050_0613039_0626240.wav\n",
      "  - segment_051.txt\n",
      "  - segment_051_0624399_0637919.wav\n",
      "  - segment_052.txt\n",
      "  - segment_052_0635519_0649519.wav\n",
      "  - segment_053.txt\n",
      "  - segment_053_0647519_0661839.wav\n",
      "  - segment_054.txt\n",
      "  - segment_054_0660078_0674240.wav\n",
      "  - segment_055.txt\n",
      "  - segment_055_0672958_0686879.wav\n",
      "  - segment_056.txt\n",
      "  - segment_056_0684958_0698159.wav\n",
      "  - segment_057.txt\n",
      "  - segment_057_0696799_0711519.wav\n",
      "  - segment_058.txt\n",
      "  - segment_058_0709600_0722800.wav\n",
      "  - segment_059.txt\n",
      "  - segment_059_0721278_0734720.wav\n",
      "  - segment_060.txt\n",
      "  - segment_060_0732240_0746958.wav\n",
      "  - segment_061.txt\n",
      "  - segment_061_0744799_0758959.wav\n",
      "  - segment_062.txt\n",
      "  - segment_062_0757200_0771360.wav\n",
      "  - segment_063.txt\n",
      "  - segment_063_0769600_0783120.wav\n",
      "  - segment_064.txt\n",
      "  - segment_064_0781519_0789120.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 0 segments\n",
      "ERROR:__main__:No segments found to index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0f67ef906e415e99160f66b274381c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error searching segments: 'SegmentVectorStore' object has no attribute 'table'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'SegmentVectorStore' object has no attribute 'table'\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "\n",
    "class SegmentVectorStore:\n",
    "    def __init__(self, segments_dir: str, db_path: str = \"segments_db\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store for audio segments\n",
    "        \"\"\"\n",
    "        self.segments_dir = segments_dir\n",
    "        self.db_path = db_path\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Initialize sentence transformer for embeddings\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "        \n",
    "        # Initialize LanceDB\n",
    "        self.db = lancedb.connect(db_path)\n",
    "        \n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def load_segments(self) -> List[Dict]:\n",
    "        \"\"\"Load all segments from the segments directory\"\"\"\n",
    "        segments = []\n",
    "        try:\n",
    "            # Check if directory exists\n",
    "            if not os.path.exists(self.segments_dir):\n",
    "                self.logger.error(f\"Directory not found: {self.segments_dir}\")\n",
    "                return segments\n",
    "\n",
    "            # Get all segment files\n",
    "            files = os.listdir(self.segments_dir)\n",
    "            audio_files = [f for f in files if f.endswith('.wav')]\n",
    "            \n",
    "            if not audio_files:\n",
    "                self.logger.warning(f\"No WAV files found in {self.segments_dir}\")\n",
    "                return segments\n",
    "\n",
    "            for audio_file in audio_files:\n",
    "                # Get corresponding text file\n",
    "                base_name = audio_file.rsplit('.', 1)[0]\n",
    "                text_file = f\"{base_name}.txt\"\n",
    "                \n",
    "                if text_file in files:\n",
    "                    try:\n",
    "                        # Parse timestamp information from filename\n",
    "                        # Format: segment_XXX_SSSSSSS_EEEEEEE.wav\n",
    "                        parts = base_name.split('_')\n",
    "                        if len(parts) >= 4:  # Ensure filename has enough parts\n",
    "                            start_time = float(parts[2]) / 1000  # Convert ms to seconds\n",
    "                            end_time = float(parts[3]) / 1000\n",
    "                            \n",
    "                            # Read text content\n",
    "                            text_path = os.path.join(self.segments_dir, text_file)\n",
    "                            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                                text = f.read().strip()\n",
    "                            \n",
    "                            segments.append({\n",
    "                                'segment_id': base_name,  # Changed from 'id' to 'segment_id'\n",
    "                                'segment_text': text,     # Changed from 'text' to 'segment_text'\n",
    "                                'audio_path': os.path.join(self.segments_dir, audio_file),\n",
    "                                'start_time': start_time,\n",
    "                                'end_time': end_time\n",
    "                            })\n",
    "                    except (IndexError, ValueError) as e:\n",
    "                        self.logger.warning(f\"Skipping malformed file {audio_file}: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, segments: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Create embeddings for segments\"\"\"\n",
    "        try:\n",
    "            if not segments:\n",
    "                self.logger.warning(\"No segments to create embeddings for\")\n",
    "                return segments\n",
    "\n",
    "            # Get text from segments\n",
    "            texts = [seg['segment_text'] for seg in segments]\n",
    "            \n",
    "            # Create embeddings\n",
    "            embeddings = self.model.encode(texts)\n",
    "            \n",
    "            # Convert embeddings to list format for PyArrow compatibility\n",
    "            for seg, emb in zip(segments, embeddings):\n",
    "                seg['vector'] = emb.tolist()  # Convert numpy array to list\n",
    "            \n",
    "            self.logger.info(f\"Created embeddings for {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def index_segments(self, table_name: str = \"segments\") -> None:\n",
    "        \"\"\"Index all segments in the vector store\"\"\"\n",
    "        try:\n",
    "            # Load segments first\n",
    "            segments = self.load_segments()\n",
    "            \n",
    "            if not segments:\n",
    "                self.logger.error(\"No segments found to index\")\n",
    "                return\n",
    "\n",
    "            # Create embeddings\n",
    "            segments_with_embeddings = self.create_embeddings(segments)\n",
    "            \n",
    "            # Define schema using segment structure\n",
    "            schema = pa.schema([\n",
    "                pa.field('segment_id', pa.string()),\n",
    "                pa.field('segment_text', pa.string()),\n",
    "                pa.field('audio_path', pa.string()),\n",
    "                pa.field('start_time', pa.float32()),\n",
    "                pa.field('end_time', pa.float32()),\n",
    "                pa.field('vector', pa.list_(pa.float32(), 384))\n",
    "            ])\n",
    "            \n",
    "            # Create table if it doesn't exist\n",
    "            if table_name not in self.db.table_names():\n",
    "                self.table = self.db.create_table(table_name, schema=schema)\n",
    "                self.logger.info(f\"Created new table: {table_name}\")\n",
    "            else:\n",
    "                self.table = self.db.open_table(table_name)\n",
    "                self.logger.info(f\"Opened existing table: {table_name}\")\n",
    "\n",
    "            # Convert to PyArrow table and add to LanceDB\n",
    "            data = pa.Table.from_pylist(segments_with_embeddings, schema=schema)\n",
    "            self.table.add(data)\n",
    "            \n",
    "            self.logger.info(f\"Indexed {len(segments)} segments in LanceDB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error indexing segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def search(self, query: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search for similar segments using a text query\"\"\"\n",
    "        try:\n",
    "            # Create query embedding\n",
    "            query_embedding = self.model.encode(query).tolist()\n",
    "            \n",
    "            # Search in LanceDB\n",
    "            results = self.table.search(query_embedding)\\\n",
    "                              .limit(limit)\\\n",
    "                              .select([\"segment_id\", \"segment_text\", \"audio_path\", \n",
    "                                     \"start_time\", \"end_time\", \"_distance\"])\\\n",
    "                              .to_list()\n",
    "            \n",
    "            self.logger.info(f\"Found {len(results)} results for query: {query}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error searching segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    # Get the current directory\n",
    "    current_dir = os.getcwd()\n",
    "    segments_dir = os.path.join(current_dir, \"segments\")\n",
    "    \n",
    "    # Print directory contents for debugging\n",
    "    print(f\"Looking for segments in: {segments_dir}\")\n",
    "    if os.path.exists(segments_dir):\n",
    "        print(\"Contents of segments directory:\")\n",
    "        for file in os.listdir(segments_dir):\n",
    "            print(f\"  - {file}\")\n",
    "    else:\n",
    "        print(\"Segments directory not found!\")\n",
    "    \n",
    "    # Initialize vector store\n",
    "    vector_store = SegmentVectorStore(\n",
    "        segments_dir=segments_dir,\n",
    "        db_path=\"segments_db\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Index segments\n",
    "        vector_store.index_segments()\n",
    "        \n",
    "        # Example search (only if segments were indexed)\n",
    "        if os.path.exists(segments_dir) and os.listdir(segments_dir):\n",
    "            query = \"example query text\"\n",
    "            results = vector_store.search(query, limit=5)\n",
    "            \n",
    "            print(\"\\nSearch Results:\")\n",
    "            for result in results:\n",
    "                print(f\"\\nSegment ID: {result['segment_id']}\")\n",
    "                print(f\"Text: {result['segment_text']}\")\n",
    "                print(f\"Audio Path: {result['audio_path']}\")\n",
    "                print(f\"Time Range: {result['start_time']:.2f}s - {result['end_time']:.2f}s\")\n",
    "                print(f\"Distance: {result['_distance']:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for segments in: c:\\Users\\shawn\\OneDrive\\Desktop\\newbie\\segments\n",
      "Contents of segments directory:\n",
      "Found 65 WAV files and 65 TXT files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 65 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e2a2e8c1ef4535bf1a829f534671a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created embeddings for 65 segments\n",
      "INFO:__main__:Opened existing table: segments\n",
      "ERROR:__main__:Error indexing segments: 'Field \"id\" does not exist in schema'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'Field \"id\" does not exist in schema'\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "import re\n",
    "\n",
    "class SegmentVectorStore:\n",
    "    def __init__(self, segments_dir: str, db_path: str = \"segments_db\"):\n",
    "        self.segments_dir = segments_dir\n",
    "        self.db_path = db_path\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "        self.db = lancedb.connect(db_path)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.table = None  # Initialize table attribute\n",
    "\n",
    "    def load_segments(self) -> List[Dict]:\n",
    "        segments = []\n",
    "        try:\n",
    "            files = os.listdir(self.segments_dir)\n",
    "            # Modified pattern to match your file format\n",
    "            pattern = r'segment_(\\d+)_(\\d+)_(\\d+)\\.wav'\n",
    "            \n",
    "            # Get all wav files\n",
    "            wav_files = [f for f in files if f.endswith('.wav')]\n",
    "            \n",
    "            for wav_file in wav_files:\n",
    "                match = re.match(pattern, wav_file)\n",
    "                if match:\n",
    "                    segment_num = match.group(1)\n",
    "                    start_time = float(match.group(2)) / 1000\n",
    "                    end_time = float(match.group(3)) / 1000\n",
    "                    \n",
    "                    # Get corresponding text file\n",
    "                    txt_file = f\"segment_{segment_num}.txt\"\n",
    "                    \n",
    "                    if txt_file in files:\n",
    "                        txt_path = os.path.join(self.segments_dir, txt_file)\n",
    "                        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                            text = f.read().strip()\n",
    "                        \n",
    "                        segments.append({\n",
    "                            'segment_id': f\"segment_{segment_num}\",\n",
    "                            'segment_text': text,\n",
    "                            'audio_path': os.path.join(self.segments_dir, wav_file),\n",
    "                            'start_time': start_time,\n",
    "                            'end_time': end_time\n",
    "                        })\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, segments: List[Dict]) -> List[Dict]:\n",
    "        try:\n",
    "            if not segments:\n",
    "                self.logger.warning(\"No segments to create embeddings for\")\n",
    "                return segments\n",
    "\n",
    "            texts = [seg['segment_text'] for seg in segments]\n",
    "            embeddings = self.model.encode(texts)\n",
    "            \n",
    "            for seg, emb in zip(segments, embeddings):\n",
    "                seg['vector'] = emb.tolist()\n",
    "            \n",
    "            self.logger.info(f\"Created embeddings for {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def index_segments(self, table_name: str = \"segments\") -> None:\n",
    "        try:\n",
    "            segments = self.load_segments()\n",
    "            \n",
    "            if not segments:\n",
    "                self.logger.error(\"No segments found to index\")\n",
    "                return\n",
    "\n",
    "            # Create embeddings\n",
    "            segments_with_embeddings = self.create_embeddings(segments)\n",
    "            \n",
    "            # Define schema\n",
    "            schema = pa.schema([\n",
    "                pa.field('segment_id', pa.string()),\n",
    "                pa.field('segment_text', pa.string()),\n",
    "                pa.field('audio_path', pa.string()),\n",
    "                pa.field('start_time', pa.float32()),\n",
    "                pa.field('end_time', pa.float32()),\n",
    "                pa.field('vector', pa.list_(pa.float32(), 384))\n",
    "            ])\n",
    "            \n",
    "            # Create or open table\n",
    "            if table_name not in self.db.table_names():\n",
    "                self.table = self.db.create_table(table_name, schema=schema)\n",
    "                self.logger.info(f\"Created new table: {table_name}\")\n",
    "            else:\n",
    "                self.table = self.db.open_table(table_name)\n",
    "                self.logger.info(f\"Opened existing table: {table_name}\")\n",
    "\n",
    "            # Convert to PyArrow table and add to LanceDB\n",
    "            data = pa.Table.from_pylist(segments_with_embeddings, schema=schema)\n",
    "            self.table.add(data)\n",
    "            \n",
    "            self.logger.info(f\"Successfully indexed {len(segments)} segments in LanceDB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error indexing segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def search(self, query: str, limit: int = 5) -> List[Dict]:\n",
    "        try:\n",
    "            if self.table is None:\n",
    "                raise ValueError(\"Table not initialized. Please run index_segments first.\")\n",
    "                \n",
    "            query_embedding = self.model.encode(query).tolist()\n",
    "            \n",
    "            results = self.table.search(query_embedding)\\\n",
    "                              .limit(limit)\\\n",
    "                              .select([\"segment_id\", \"segment_text\", \"audio_path\", \n",
    "                                     \"start_time\", \"end_time\", \"_distance\"])\\\n",
    "                              .to_list()\n",
    "            \n",
    "            self.logger.info(f\"Found {len(results)} results for query: {query}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error searching segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage with better error handling\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    current_dir = os.getcwd()\n",
    "    segments_dir = os.path.join(current_dir, \"segments\")\n",
    "    \n",
    "    print(f\"Looking for segments in: {segments_dir}\")\n",
    "    if os.path.exists(segments_dir):\n",
    "        print(\"Contents of segments directory:\")\n",
    "        files = os.listdir(segments_dir)\n",
    "        wav_files = [f for f in files if f.endswith('.wav')]\n",
    "        txt_files = [f for f in files if f.endswith('.txt')]\n",
    "        print(f\"Found {len(wav_files)} WAV files and {len(txt_files)} TXT files\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize vector store\n",
    "        vector_store = SegmentVectorStore(\n",
    "            segments_dir=segments_dir,\n",
    "            db_path=\"segments_db\"\n",
    "        )\n",
    "        \n",
    "        # Index segments\n",
    "        vector_store.index_segments()\n",
    "        \n",
    "        # Only proceed with search if indexing was successful\n",
    "        if vector_store.table is not None:\n",
    "            query = \"what is this video about?\"\n",
    "            results = vector_store.search(query, limit=5)\n",
    "            \n",
    "            print(\"\\nSearch Results:\")\n",
    "            for result in results:\n",
    "                print(f\"\\nSegment ID: {result['segment_id']}\")\n",
    "                print(f\"Text: {result['segment_text']}\")\n",
    "                print(f\"Time Range: {result['start_time']:.2f}s - {result['end_time']:.2f}s\")\n",
    "                print(f\"Similarity Score: {1 - result['_distance']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Loaded 65 segments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing segments...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b92e08f5ed74250a4be1095cc351668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created embeddings for 65 segments\n",
      "INFO:__main__:Opened existing table: segments\n",
      "ERROR:__main__:Error indexing segments: 'Field \"id\" does not exist in schema'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during testing: 'Field \"id\" does not exist in schema'\n"
     ]
    }
   ],
   "source": [
    "# Test the vector store with proper error handling\n",
    "try:\n",
    "\t# Initialize vector store with existing segments directory\n",
    "\tvector_store = SegmentVectorStore(\n",
    "\t\tsegments_dir=segments_dir,\n",
    "\t\tdb_path=\"segments_db\"\n",
    "\t)\n",
    "\t\n",
    "\t# Index the segments\n",
    "\tprint(\"Indexing segments...\")\n",
    "\tvector_store.index_segments()\n",
    "\t\n",
    "\t# Test search functionality with different queries\n",
    "\ttest_queries = [\n",
    "\t\t\"What is computer science?\",\n",
    "\t\t\"What is a Turing machine?\",\n",
    "\t\t\"How does object oriented programming work?\",\n",
    "\t\t\"Explain algorithms\"\n",
    "\t]\n",
    "\t\n",
    "\tprint(\"\\nTesting search functionality with multiple queries:\")\n",
    "\tfor query in test_queries:\n",
    "\t\tprint(f\"\\nQuery: {query}\")\n",
    "\t\tresults = vector_store.search(query, limit=3)\n",
    "\t\t\n",
    "\t\tfor i, result in enumerate(results, 1):\n",
    "\t\t\tprint(f\"\\nResult {i}:\")\n",
    "\t\t\tprint(f\"Segment: {result['segment_id']}\")\n",
    "\t\t\tprint(f\"Text: {result['segment_text'][:200]}...\")  # Show first 200 chars\n",
    "\t\t\tprint(f\"Time Range: {result['start_time']:.2f}s - {result['end_time']:.2f}s\")\n",
    "\t\t\tprint(f\"Similarity Score: {1 - result['_distance']:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "\tprint(f\"Error during testing: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "import re\n",
    "\n",
    "class SegmentVectorStore:\n",
    "    def __init__(self, segments_dir: str, db_path: str = \"segments_db\"):\n",
    "        self.segments_dir = segments_dir\n",
    "        self.db_path = db_path\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "        self.db = lancedb.connect(db_path)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.table = None\n",
    "\n",
    "    def load_segments(self) -> List[Dict]:\n",
    "        segments = []\n",
    "        try:\n",
    "            files = os.listdir(self.segments_dir)\n",
    "            wav_files = [f for f in files if f.endswith('.wav')]\n",
    "            \n",
    "            for wav_file in wav_files:\n",
    "                # Extract segment number from filename\n",
    "                segment_num = wav_file.split('_')[1]  # Get the number after \"segment_\"\n",
    "                txt_file = f\"{wav_file.rsplit('_', 2)[0]}.txt\"  # Remove timestamp parts\n",
    "                \n",
    "                if txt_file in files:\n",
    "                    txt_path = os.path.join(self.segments_dir, txt_file)\n",
    "                    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                        text = f.read().strip()\n",
    "                    \n",
    "                    # Extract timestamps from wav filename\n",
    "                    parts = wav_file.split('_')\n",
    "                    if len(parts) >= 4:\n",
    "                        start_time = float(parts[-2]) / 1000\n",
    "                        end_time = float(parts[-1].split('.')[0]) / 1000\n",
    "                    else:\n",
    "                        start_time = 0.0\n",
    "                        end_time = 0.0\n",
    "                    \n",
    "                    segments.append({\n",
    "                        'id': f\"segment_{segment_num}\",  # Changed from segment_id to id\n",
    "                        'segment_text': text,\n",
    "                        'audio_path': os.path.join(self.segments_dir, wav_file),\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time\n",
    "                    })\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, segments: List[Dict]) -> List[Dict]:\n",
    "        try:\n",
    "            if not segments:\n",
    "                self.logger.warning(\"No segments to create embeddings for\")\n",
    "                return segments\n",
    "\n",
    "            texts = [seg['segment_text'] for seg in segments]\n",
    "            embeddings = self.model.encode(texts)\n",
    "            \n",
    "            for seg, emb in zip(segments, embeddings):\n",
    "                seg['vector'] = emb.tolist()\n",
    "            \n",
    "            self.logger.info(f\"Created embeddings for {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def index_segments(self, table_name: str = \"segments\") -> None:\n",
    "        try:\n",
    "            segments = self.load_segments()\n",
    "            \n",
    "            if not segments:\n",
    "                self.logger.error(\"No segments found to index\")\n",
    "                return\n",
    "\n",
    "            segments_with_embeddings = self.create_embeddings(segments)\n",
    "            \n",
    "            # Updated schema with 'id' instead of 'segment_id'\n",
    "            schema = pa.schema([\n",
    "                pa.field('id', pa.string()),  # Changed from segment_id to id\n",
    "                pa.field('segment_text', pa.string()),\n",
    "                pa.field('audio_path', pa.string()),\n",
    "                pa.field('start_time', pa.float32()),\n",
    "                pa.field('end_time', pa.float32()),\n",
    "                pa.field('vector', pa.list_(pa.float32(), 384))\n",
    "            ])\n",
    "            \n",
    "            # Drop existing table if it exists\n",
    "            if table_name in self.db.table_names():\n",
    "                self.db.drop_table(table_name)\n",
    "            \n",
    "            # Create new table\n",
    "            self.table = self.db.create_table(\n",
    "                table_name,\n",
    "                data=segments_with_embeddings,\n",
    "                schema=schema,\n",
    "                mode=\"overwrite\"\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Successfully indexed {len(segments)} segments in LanceDB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error indexing segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def search(self, query: str, limit: int = 5) -> List[Dict]:\n",
    "        try:\n",
    "            if self.table is None:\n",
    "                raise ValueError(\"Table not initialized. Please run index_segments first.\")\n",
    "                \n",
    "            query_embedding = self.model.encode(query).tolist()\n",
    "            \n",
    "            results = self.table.search(query_embedding)\\\n",
    "                              .limit(limit)\\\n",
    "                              .select([\"id\", \"segment_text\", \"audio_path\", \n",
    "                                     \"start_time\", \"end_time\", \"_distance\"])\\\n",
    "                              .to_list()\n",
    "            \n",
    "            self.logger.info(f\"Found {len(results)} results for query: {query}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error searching segments: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Loaded 65 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e26b84c2ac940b6a9698bda345f1b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created embeddings for 65 segments\n",
      "INFO:__main__:Successfully indexed 65 segments in LanceDB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ab4f189165475d8c0219ded4bda1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error searching segments: LanceError(Schema): Column _distance does not exist, D:\\a\\lance\\lance\\rust\\lance-core\\src\\datatypes\\schema.rs:190:31\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "LanceError(Schema): Column _distance does not exist, D:\\a\\lance\\lance\\rust\\lance-core\\src\\datatypes\\schema.rs:190:31",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m SegmentVectorStore(segments_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegments\u001b[39m\u001b[38;5;124m\"\u001b[39m, db_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegments_db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m vector_store\u001b[38;5;241m.\u001b[39mindex_segments()\n\u001b[1;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mturing machine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 129\u001b[0m, in \u001b[0;36mSegmentVectorStore.search\u001b[1;34m(self, query, limit)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable not initialized. Please run index_segments first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    123\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencode(query)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    125\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegment_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_distance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m--> 129\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m results for query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lancedb\\query.py:333\u001b[0m, in \u001b[0;36mLanceQueryBuilder.to_list\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_list\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    Execute the query and return the results as a list of dictionaries.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m    fields are returned whether or not they're explicitly selected.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pylist()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lancedb\\query.py:662\u001b[0m, in \u001b[0;36mLanceVectorQueryBuilder.to_arrow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_arrow\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable:\n\u001b[0;32m    654\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;124;03m    Execute the query and return the results as an\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;124;03m    [Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;124;03m    vector and the returned vectors.\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread_all()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lancedb\\query.py:694\u001b[0m, in \u001b[0;36mLanceVectorQueryBuilder.to_batches\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    679\u001b[0m     vector \u001b[38;5;241m=\u001b[39m [v\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vector]\n\u001b[0;32m    680\u001b[0m query \u001b[38;5;241m=\u001b[39m Query(\n\u001b[0;32m    681\u001b[0m     vector\u001b[38;5;241m=\u001b[39mvector,\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_where,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m     fast_search\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_search,\n\u001b[0;32m    693\u001b[0m )\n\u001b[1;32m--> 694\u001b[0m result_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reranker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    696\u001b[0m     rs_table \u001b[38;5;241m=\u001b[39m result_set\u001b[38;5;241m.\u001b[39mread_all()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lancedb\\table.py:1892\u001b[0m, in \u001b[0;36mLanceTable._execute_query\u001b[1;34m(self, query, batch_size)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query\u001b[38;5;241m.\u001b[39mvector) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1884\u001b[0m     nearest \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1885\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m: query\u001b[38;5;241m.\u001b[39mvector_column,\n\u001b[0;32m   1886\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m: query\u001b[38;5;241m.\u001b[39mvector,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1890\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefine_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m: query\u001b[38;5;241m.\u001b[39mrefine_factor,\n\u001b[0;32m   1891\u001b[0m     }\n\u001b[1;32m-> 1892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscanner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefilter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefilter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnearest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnearest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_text_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_text_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_row_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_row_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_reader()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lance\\dataset.py:389\u001b[0m, in \u001b[0;36mLanceDataset.scanner\u001b[1;34m(self, columns, filter, limit, offset, nearest, batch_size, batch_readahead, fragment_readahead, scan_in_order, fragments, full_text_query, prefilter, with_row_id, with_row_address, use_stats, fast_search, io_buffer_size, late_materialization, use_scalar_index)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nearest \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     builder \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mnearest(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnearest)\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_scanner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lance\\dataset.py:2784\u001b[0m, in \u001b[0;36mScannerBuilder.to_scanner\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_scanner\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LanceScanner:\n\u001b[1;32m-> 2784\u001b[0m     scanner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscanner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2785\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2786\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_columns_with_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2787\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2788\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prefilter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2789\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2790\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2791\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nearest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2792\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2793\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2794\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_readahead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fragment_readahead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2796\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scan_in_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2797\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fragments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_row_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_row_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2800\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2801\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_substrait_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2802\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2803\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_text_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2804\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_late_materialization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2805\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_scalar_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2806\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LanceScanner(scanner, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds)\n",
      "\u001b[1;31mValueError\u001b[0m: LanceError(Schema): Column _distance does not exist, D:\\a\\lance\\lance\\rust\\lance-core\\src\\datatypes\\schema.rs:190:31"
     ]
    }
   ],
   "source": [
    "vector_store = SegmentVectorStore(segments_dir=\"segments\", db_path=\"segments_db\")\n",
    "vector_store.index_segments()\n",
    "results = vector_store.search(\"turing machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Loaded 65 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990653a094d945f7ad91cb52c3a99a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created embeddings for 65 segments\n",
      "ERROR:__main__:Error indexing segments: Metric ['vector'] not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Metric ['vector'] not supported.\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "import re\n",
    "\n",
    "class SegmentVectorStore:\n",
    "    def __init__(self, segments_dir: str, db_path: str = \"segments_db\"):\n",
    "        self.segments_dir = segments_dir\n",
    "        self.db_path = db_path\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "        self.db = lancedb.connect(db_path)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.table = None\n",
    "\n",
    "    def load_segments(self) -> List[Dict]:\n",
    "        segments = []\n",
    "        try:\n",
    "            files = os.listdir(self.segments_dir)\n",
    "            wav_files = [f for f in files if f.endswith('.wav')]\n",
    "            \n",
    "            for wav_file in wav_files:\n",
    "                # Extract segment number from filename\n",
    "                segment_num = wav_file.split('_')[1]  # Get the number after \"segment_\"\n",
    "                txt_file = f\"{wav_file.rsplit('_', 2)[0]}.txt\"  # Remove timestamp parts\n",
    "                \n",
    "                if txt_file in files:\n",
    "                    txt_path = os.path.join(self.segments_dir, txt_file)\n",
    "                    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                        text = f.read().strip()\n",
    "                    \n",
    "                    # Extract timestamps from wav filename\n",
    "                    parts = wav_file.split('_')\n",
    "                    if len(parts) >= 4:\n",
    "                        start_time = float(parts[-2]) / 1000\n",
    "                        end_time = float(parts[-1].split('.')[0]) / 1000\n",
    "                    else:\n",
    "                        start_time = 0.0\n",
    "                        end_time = 0.0\n",
    "                    \n",
    "                    segments.append({\n",
    "                        'id': f\"segment_{segment_num}\",\n",
    "                        'segment_text': text,\n",
    "                        'audio_path': os.path.join(self.segments_dir, wav_file),\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time\n",
    "                    })\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, segments: List[Dict]) -> List[Dict]:\n",
    "        try:\n",
    "            if not segments:\n",
    "                self.logger.warning(\"No segments to create embeddings for\")\n",
    "                return segments\n",
    "\n",
    "            texts = [seg['segment_text'] for seg in segments]\n",
    "            embeddings = self.model.encode(texts)\n",
    "            \n",
    "            for seg, emb in zip(segments, embeddings):\n",
    "                seg['vector'] = emb.tolist()\n",
    "            \n",
    "            self.logger.info(f\"Created embeddings for {len(segments)} segments\")\n",
    "            return segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def index_segments(self, table_name: str = \"segments\") -> None:\n",
    "        try:\n",
    "            segments = self.load_segments()\n",
    "            \n",
    "            if not segments:\n",
    "                self.logger.error(\"No segments found to index\")\n",
    "                return\n",
    "\n",
    "            segments_with_embeddings = self.create_embeddings(segments)\n",
    "            \n",
    "            schema = pa.schema([\n",
    "                pa.field('id', pa.string()),\n",
    "                pa.field('segment_text', pa.string()),\n",
    "                pa.field('audio_path', pa.string()),\n",
    "                pa.field('start_time', pa.float32()),\n",
    "                pa.field('end_time', pa.float32()),\n",
    "                pa.field('vector', pa.list_(pa.float32(), 384))\n",
    "            ])\n",
    "            \n",
    "            # Drop existing table if it exists\n",
    "            if table_name in self.db.table_names():\n",
    "                self.db.drop_table(table_name)\n",
    "            \n",
    "            # Create new table with vector column configuration\n",
    "            self.table = self.db.create_table(\n",
    "                table_name,\n",
    "                data=segments_with_embeddings,\n",
    "                schema=schema,\n",
    "                mode=\"overwrite\"\n",
    "            )\n",
    "            \n",
    "            # Create vector index\n",
    "            self.table.create_index([\"vector\"])\n",
    "            \n",
    "            self.logger.info(f\"Successfully indexed {len(segments)} segments in LanceDB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error indexing segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def search(self, query: str, limit: int = 5) -> List[Dict]:\n",
    "        try:\n",
    "            if self.table is None:\n",
    "                raise ValueError(\"Table not initialized. Please run index_segments first.\")\n",
    "                \n",
    "            query_embedding = self.model.encode(query).tolist()\n",
    "            \n",
    "            # Use vector_column parameter and don't select _distance\n",
    "            results = self.table.search(\n",
    "                query_embedding,\n",
    "                vector_column_name=\"vector\",\n",
    "                limit=limit\n",
    "            ).select([\n",
    "                \"id\",\n",
    "                \"segment_text\",\n",
    "                \"audio_path\",\n",
    "                \"start_time\",\n",
    "                \"end_time\"\n",
    "            ]).to_list()\n",
    "            \n",
    "            self.logger.info(f\"Found {len(results)} results for query: {query}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error searching segments: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    try:\n",
    "        vector_store = SegmentVectorStore(segments_dir=\"segments\", db_path=\"segments_db\")\n",
    "        vector_store.index_segments()\n",
    "        \n",
    "        # Test search\n",
    "        if vector_store.table is not None:\n",
    "            results = vector_store.search(\"what is this video about?\", limit=5)\n",
    "            print(\"\\nSearch Results:\")\n",
    "            for result in results:\n",
    "                print(f\"\\nSegment ID: {result['id']}\")\n",
    "                print(f\"Text: {result['segment_text']}\")\n",
    "                print(f\"Time Range: {result['start_time']:.2f}s - {result['end_time']:.2f}s\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Loaded 65 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1feb01410034376821f775c622f674f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created embeddings for 65 segments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: LanceDBConnection.create_table() got an unexpected keyword argument 'index_config'\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store and create table with proper vector index\n",
    "vector_store = SegmentVectorStore(segments_dir=segments_dir, db_path=\"segments_db\")\n",
    "\n",
    "try:\n",
    "    # Drop existing table if it exists\n",
    "    if \"segments\" in vector_store.db.table_names():\n",
    "        vector_store.db.drop_table(\"segments\")\n",
    "    \n",
    "    # Load and process segments\n",
    "    segments = vector_store.load_segments()\n",
    "    segments_with_embeddings = vector_store.create_embeddings(segments)\n",
    "    \n",
    "    # Create table with vector column configuration\n",
    "    vector_store.table = vector_store.db.create_table(\n",
    "        name=\"segments\",\n",
    "        data=segments_with_embeddings,\n",
    "        mode=\"overwrite\",\n",
    "        index_config={\n",
    "            \"vector\": {\n",
    "                \"type\": \"FLAT\",\n",
    "                \"metric_type\": \"L2\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Test search functionality\n",
    "    results = vector_store.search(\"what is computer science\", limit=3)\n",
    "    \n",
    "    print(\"\\nSearch Results:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"Segment: {result['id']}\")\n",
    "        print(f\"Text: {result['segment_text'][:200]}...\")  # Show first 200 chars\n",
    "        print(f\"Time Range: {result['start_time']:.2f}s - {result['end_time']:.2f}s\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 65 segments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vector store...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbf60a092f74e7d9a76261ae18c7b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created embeddings for 65 segments\n",
      "ERROR:__main__:Error indexing segments: Metric ['vector'] not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during search: Metric ['vector'] not supported.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d13cdbd4efa4af3acbd79eb6314cfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error searching segments: LanceTable.search() got an unexpected keyword argument 'limit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during search: LanceTable.search() got an unexpected keyword argument 'limit'\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4358f474b8c94eeeb20a49fa5a447336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error searching segments: LanceTable.search() got an unexpected keyword argument 'limit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during search: LanceTable.search() got an unexpected keyword argument 'limit'\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455d6c1ebf4441b0adc6f08a2f9f1a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error searching segments: LanceTable.search() got an unexpected keyword argument 'limit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during search: LanceTable.search() got an unexpected keyword argument 'limit'\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_segments(query: str, limit: int = 3):\n",
    "\t\"\"\"\n",
    "\tQuery the vector store and format results in a readable way\n",
    "\t\"\"\"\n",
    "\ttry:\n",
    "\t\t# Ensure vector store is initialized\n",
    "\t\tif not hasattr(vector_store, 'table') or vector_store.table is None:\n",
    "\t\t\tprint(\"Initializing vector store...\")\n",
    "\t\t\tvector_store.index_segments()\n",
    "\t\t\n",
    "\t\t# Perform search\n",
    "\t\tresults = vector_store.search(query, limit=limit)\n",
    "\t\t\n",
    "\t\tif not results:\n",
    "\t\t\tprint(\"No relevant segments found.\")\n",
    "\t\t\treturn\n",
    "\t\t\t\n",
    "\t\tprint(f\"\\nSearch Results for: '{query}'\\n\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\t\t\n",
    "\t\tfor i, result in enumerate(results, 1):\n",
    "\t\t\tprint(f\"\\nResult {i}:\")\n",
    "\t\t\tprint(f\"Segment: {result['id']}\")\n",
    "\t\t\tprint(f\"Time Range: {result['start_time']:.2f}s - {result['end_time']:.2f}s\")\n",
    "\t\t\tprint(f\"Text: {result['segment_text']}\")\n",
    "\t\t\tprint(\"-\" * 80)\n",
    "\t\t\t\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error during search: {str(e)}\")\n",
    "\n",
    "# Test the function with some example queries\n",
    "test_queries = [\n",
    "\t\"What is computer science?\",\n",
    "\t\"How does memory work in computers?\",\n",
    "\t\"Explain data structures\",\n",
    "\t\"What is object oriented programming?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "\tquery_segments(query)\n",
    "\tprint(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing semantic search with example queries...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386d6ad05e484eae8b46c2a89d155e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error searching segments: LanceTable.search() got an unexpected keyword argument 'limit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during search: LanceTable.search() got an unexpected keyword argument 'limit'\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a050dfebb684d35a3bfbcf9a2b43c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error searching segments: LanceTable.search() got an unexpected keyword argument 'limit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during search: LanceTable.search() got an unexpected keyword argument 'limit'\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3615fd9cbd4c85b3ec0ac96ab790b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error searching segments: LanceTable.search() got an unexpected keyword argument 'limit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during search: LanceTable.search() got an unexpected keyword argument 'limit'\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510bf98be362498c9d1d04692547a2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error searching segments: LanceTable.search() got an unexpected keyword argument 'limit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during search: LanceTable.search() got an unexpected keyword argument 'limit'\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_semantic_search_ui():\n",
    "\t# Helper function to format search results\n",
    "\tdef format_result(result):\n",
    "\t\t\"\"\"Format a single search result for display\"\"\"\n",
    "\t\treturn f\"\"\"\n",
    "Time: {result['start_time']:.2f}s - {result['end_time']:.2f}s\n",
    "Text: {result['segment_text']}\n",
    "Audio: {os.path.basename(result['audio_path'])}\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "\t# Function to handle search\n",
    "\tdef search_segments(query, limit=5, threshold=0.6):\n",
    "\t\ttry:\n",
    "\t\t\t# Ensure vector store is available\n",
    "\t\t\tif not hasattr(vector_store, 'table') or vector_store.table is None:\n",
    "\t\t\t\tprint(\"Initializing vector store...\")\n",
    "\t\t\t\tvector_store.index_segments()\n",
    "\n",
    "\t\t\t# Perform search \n",
    "\t\t\tresults = vector_store.search(query, limit=limit)\n",
    "\t\t\t\n",
    "\t\t\tif not results:\n",
    "\t\t\t\treturn \"No results found.\"\n",
    "\t\t\t\t\n",
    "\t\t\t# Format output\n",
    "\t\t\toutput = f\"\\nSearch Results for: '{query}'\\n\"\n",
    "\t\t\toutput += \"=\"*80 + \"\\n\"\n",
    "\t\t\tfor i, result in enumerate(results, 1):\n",
    "\t\t\t\toutput += f\"\\nResult {i}:\\n\"\n",
    "\t\t\t\toutput += format_result(result)\n",
    "\t\t\t\n",
    "\t\t\treturn output\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\treturn f\"Error during search: {str(e)}\"\n",
    "\n",
    "\t# Test queries\n",
    "\tprint(\"Testing semantic search with example queries...\\n\")\n",
    "\t\n",
    "\ttest_queries = [\n",
    "\t\t\"What is object oriented programming?\",\n",
    "\t\t\"How does memory work in computers?\",\n",
    "\t\t\"Explain data structures\",\n",
    "\t\t\"What are algorithms?\"\n",
    "\t]\n",
    "\n",
    "\tfor query in test_queries:\n",
    "\t\tprint(search_segments(query, limit=3))\n",
    "\t\tprint(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run the semantic search UI\n",
    "create_semantic_search_ui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Creating table...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325529896c434d1b9d9638b4ca0069f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eea15d3f0ef473e9981ac9c47f038c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066a520ff37c47df9df895cffe65c9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6752761f7a4f4b5a8c99a63a982a6a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Table created successfully\n",
      "INFO:__main__:Searching for: what is this about?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b970964ece294f97a57b332eb6e320ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results:\n",
      "\n",
      "Segment segment_002:\n",
      "Text: [Music] welcome to computer science 101 in today's video you'll learn the science behind the garbage code you've been writing by learning 101 different computer science terms and concepts this is a computer it's just a piece of tape\n",
      "Audio File: segment_002_0023519_0037679.wav\n",
      "\n",
      "Segment segment_009:\n",
      "Text: applications for that computers have random access memory or ram it's like a neighborhood and inside of every house lives a byte every location has a memory address which the cpu can read and write to you can think of the cpu and ram as\n",
      "Audio File: segment_009_0108640_0123359.wav\n",
      "\n",
      "Segment segment_017:\n",
      "Text: application is to declare a variable this attaches a name to a data point allowing you to reuse it somewhere else in your code python is a dynamically typed language which means we don't need to tell the program exactly which data\n",
      "Audio File: segment_017_0206878_0220799.wav\n",
      "\n",
      "Segment segment_048:\n",
      "Text: what the program does and the outcome but doesn't care about things like control flow this style of programming is often associated with functional languages like haskell the other paradigm is imperative programming where your code uses statements like if and\n",
      "Audio File: segment_048_0588399_0602879.wav\n",
      "\n",
      "Segment segment_050:\n",
      "Text: means they support all these options at the same time in addition to object-oriented programming the idea behind oop is that you use classes to write a blueprint for the data or objects in your code a class can\n",
      "Audio File: segment_050_0613039_0626240.wav\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import logging\n",
    "import pyarrow as pa\n",
    "from typing import List, Generator\n",
    "\n",
    "class SegmentVectorStore:\n",
    "    def __init__(self, segments_dir: str):\n",
    "        self.segments_dir = segments_dir\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.db = lancedb.connect(\"segments.db\")\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.vector_dim = 384  # MiniLM-L6-v2 embedding dimension\n",
    "        \n",
    "    def load_segments(self) -> List[dict]:\n",
    "        segments = []\n",
    "        files = os.listdir(self.segments_dir)\n",
    "        wav_files = [f for f in files if f.endswith('.wav')]\n",
    "        \n",
    "        for wav_file in wav_files:\n",
    "            base_name = wav_file.split('_')[1]\n",
    "            txt_file = f\"segment_{base_name}.txt\"\n",
    "            \n",
    "            if txt_file in files:\n",
    "                with open(os.path.join(self.segments_dir, txt_file), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read().strip()\n",
    "                \n",
    "                segments.append({\n",
    "                    \"text\": text,\n",
    "                    \"id\": f\"segment_{base_name}\",\n",
    "                    \"audio_file\": wav_file\n",
    "                })\n",
    "        \n",
    "        return segments\n",
    "        \n",
    "    def create_batches(self, batch_size: int = 20) -> Generator[pa.RecordBatch, None, None]:\n",
    "        segments = self.load_segments()\n",
    "        \n",
    "        # Process segments in batches\n",
    "        for i in range(0, len(segments), batch_size):\n",
    "            batch_segments = segments[i:i + batch_size]\n",
    "            \n",
    "            # Create embeddings for the batch\n",
    "            texts = [s[\"text\"] for s in batch_segments]\n",
    "            embeddings = self.model.encode(texts)\n",
    "            \n",
    "            # Prepare arrays for PyArrow\n",
    "            vectors = pa.array(embeddings.tolist(), pa.list_(pa.float32(), self.vector_dim))\n",
    "            ids = pa.array([s[\"id\"] for s in batch_segments], pa.string())\n",
    "            texts = pa.array([s[\"text\"] for s in batch_segments], pa.string())\n",
    "            audio_files = pa.array([s[\"audio_file\"] for s in batch_segments], pa.string())\n",
    "            \n",
    "            # Create and yield batch\n",
    "            yield pa.RecordBatch.from_arrays(\n",
    "                [vectors, ids, texts, audio_files],\n",
    "                [\"vector\", \"id\", \"text\", \"audio_file\"]\n",
    "            )\n",
    "    \n",
    "    def create_table(self):\n",
    "        # Define schema\n",
    "        schema = pa.schema([\n",
    "            pa.field(\"vector\", pa.list_(pa.float32(), self.vector_dim)),\n",
    "            pa.field(\"id\", pa.string()),\n",
    "            pa.field(\"text\", pa.string()),\n",
    "            pa.field(\"audio_file\", pa.string())\n",
    "        ])\n",
    "        \n",
    "        # Drop existing table if it exists\n",
    "        if \"segments\" in self.db.table_names():\n",
    "            self.db.drop_table(\"segments\")\n",
    "        \n",
    "        # Create new table with batches\n",
    "        table = self.db.create_table(\n",
    "            \"segments\",\n",
    "            self.create_batches(),\n",
    "            schema=schema,\n",
    "            mode=\"create\"\n",
    "        )\n",
    "        \n",
    "        return table\n",
    "    \n",
    "    def search(self, query: str, limit: int = 5):\n",
    "        table = self.db.open_table(\"segments\")\n",
    "        query_vector = self.model.encode(query).tolist()\n",
    "        \n",
    "        results = table.search(\n",
    "            query_vector,\n",
    "            vector_column_name=\"vector\"\n",
    "        ).limit(limit).to_list()\n",
    "        \n",
    "        return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        store = SegmentVectorStore(\"segments\")\n",
    "        \n",
    "        # Create table with batches\n",
    "        logger.info(\"Creating table...\")\n",
    "        table = store.create_table()\n",
    "        logger.info(\"Table created successfully\")\n",
    "        \n",
    "        # Test search\n",
    "        query = \"what is this about?\"\n",
    "        logger.info(f\"Searching for: {query}\")\n",
    "        results = store.search(query)\n",
    "        \n",
    "        print(\"\\nSearch Results:\")\n",
    "        for r in results:\n",
    "            print(f\"\\nSegment {r['id']}:\")\n",
    "            print(f\"Text: {r['text']}\")\n",
    "            print(f\"Audio File: {r['audio_file']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching for: what is this about?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98e476e12d94eaf9e22f48c45af46f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "Segment segment_002:\n",
      "Text: [Music] welcome to computer science 101 in today's video you'll learn the science behind the garbage code you've been writing by learning 101 different computer science terms and concepts this is a computer it's just a piece of tape\n",
      "Audio File: segment_002_0023519_0037679.wav\n",
      "\n",
      "Segment segment_009:\n",
      "Text: applications for that computers have random access memory or ram it's like a neighborhood and inside of every house lives a byte every location has a memory address which the cpu can read and write to you can think of the cpu and ram as\n",
      "Audio File: segment_009_0108640_0123359.wav\n",
      "\n",
      "Segment segment_017:\n",
      "Text: application is to declare a variable this attaches a name to a data point allowing you to reuse it somewhere else in your code python is a dynamically typed language which means we don't need to tell the program exactly which data\n",
      "Audio File: segment_017_0206878_0220799.wav\n",
      "\n",
      "Segment segment_048:\n",
      "Text: what the program does and the outcome but doesn't care about things like control flow this style of programming is often associated with functional languages like haskell the other paradigm is imperative programming where your code uses statements like if and\n",
      "Audio File: segment_048_0588399_0602879.wav\n",
      "\n",
      "Segment segment_050:\n",
      "Text: means they support all these options at the same time in addition to object-oriented programming the idea behind oop is that you use classes to write a blueprint for the data or objects in your code a class can\n",
      "Audio File: segment_050_0613039_0626240.wav\n",
      "\n",
      "Searching for: can you explain the main topic?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9284012ae64cefac0d53e5bf0f2992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "Segment segment_002:\n",
      "Text: [Music] welcome to computer science 101 in today's video you'll learn the science behind the garbage code you've been writing by learning 101 different computer science terms and concepts this is a computer it's just a piece of tape\n",
      "Audio File: segment_002_0023519_0037679.wav\n",
      "\n",
      "Segment segment_009:\n",
      "Text: applications for that computers have random access memory or ram it's like a neighborhood and inside of every house lives a byte every location has a memory address which the cpu can read and write to you can think of the cpu and ram as\n",
      "Audio File: segment_009_0108640_0123359.wav\n",
      "\n",
      "Segment segment_052:\n",
      "Text: inheritance where a subclass can extend and override the behaviors of the parent class and it opens the door to all kinds of other ideas called design patterns now a class by itself doesn't actually do anything instead it's used to\n",
      "Audio File: segment_052_0635519_0649519.wav\n",
      "\n",
      "Segment segment_004:\n",
      "Text: central processing unit if we crack it open we find a piece of silicon that contains billions of tiny transistors which are like microscopic on off switches the value at one of these switches is called a bit and is the smallest piece of information a computer\n",
      "Audio File: segment_004_0048479_0063280.wav\n",
      "\n",
      "Segment segment_011:\n",
      "Text: together because we have operating system kernels like linux mac and windows that control all hardware resources via device drivers now to start hacking on the operating system your first entry point is the shell which is a program that exposes the\n",
      "Audio File: segment_011_0133520_0148400.wav\n",
      "\n",
      "Searching for: tell me more about the details\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1729d4a721d34bc9babd3d3ec5d4e621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "Segment segment_011:\n",
      "Text: together because we have operating system kernels like linux mac and windows that control all hardware resources via device drivers now to start hacking on the operating system your first entry point is the shell which is a program that exposes the\n",
      "Audio File: segment_011_0133520_0148400.wav\n",
      "\n",
      "Segment segment_001:\n",
      "Text: learn to code and get a high paying job while literally having no idea how anything actually works it all just feels like magic like a pilot driving a giant metal tube in the sky while knowing nothing about aerodynamics\n",
      "Audio File: segment_001_0011279_0024719.wav\n",
      "\n",
      "Segment segment_009:\n",
      "Text: applications for that computers have random access memory or ram it's like a neighborhood and inside of every house lives a byte every location has a memory address which the cpu can read and write to you can think of the cpu and ram as\n",
      "Audio File: segment_009_0108640_0123359.wav\n",
      "\n",
      "Segment segment_060:\n",
      "Text: address is usually alias to a url that is registered in a global database called the domain name service now to establish a connection the two computers will perform a tcp handshake which will allow them to exchange messages called\n",
      "Audio File: segment_060_0732240_0746958.wav\n",
      "\n",
      "Segment segment_061:\n",
      "Text: packets on top of that there's usually a security layer like ssl to encrypt and decrypt the messages over the network now the two computers can securely share data with the hypertext transfer protocol the client may request a web\n",
      "Audio File: segment_061_0744799_0758959.wav\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SimpleRetriever:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.db = lancedb.connect(\"segments.db\")\n",
    "        \n",
    "    def search(self, query: str, limit: int = 5):\n",
    "        table = self.db.open_table(\"segments\")\n",
    "        query_vector = self.model.encode(query).tolist()\n",
    "        \n",
    "        results = table.search(\n",
    "            query_vector,\n",
    "            vector_column_name=\"vector\"\n",
    "        ).limit(limit).to_list()\n",
    "        \n",
    "        return results\n",
    "\n",
    "def test_retriever():\n",
    "    # Initialize retriever\n",
    "    retriever = SimpleRetriever()\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"what is this about?\",\n",
    "        \"can you explain the main topic?\",\n",
    "        \"tell me more about the details\"\n",
    "    ]\n",
    "    \n",
    "    # Run searches\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nSearching for: {query}\")\n",
    "        results = retriever.search(query)\n",
    "        \n",
    "        print(\"Results:\")\n",
    "        for r in results:\n",
    "            print(f\"\\nSegment {r['id']}:\")\n",
    "            print(f\"Text: {r['text']}\")\n",
    "            print(f\"Audio File: {r['audio_file']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\shawn\\appdata\\roaming\\python\\python312\\site-packages (from sounddevice) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shawn\\appdata\\roaming\\python\\python312\\site-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
      "Downloading sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Run test queries\n",
      "2. Interactive search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching for: what is this about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shawn\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd108553daf8429bb49f90661648450f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "Segment segment_002:\n",
      "Text: [Music] welcome to computer science 101 in today's video you'll learn the science behind the garbage code you've been writing by learning 101 different computer science terms and concepts this is a computer it's just a piece of tape\n",
      "Audio File: segment_002_0023519_0037679.wav\n",
      "\n",
      "Playing audio: segment_002_0023519_0037679.wav\n",
      "\n",
      "Segment segment_009:\n",
      "Text: applications for that computers have random access memory or ram it's like a neighborhood and inside of every house lives a byte every location has a memory address which the cpu can read and write to you can think of the cpu and ram as\n",
      "Audio File: segment_009_0108640_0123359.wav\n",
      "\n",
      "Segment segment_017:\n",
      "Text: application is to declare a variable this attaches a name to a data point allowing you to reuse it somewhere else in your code python is a dynamically typed language which means we don't need to tell the program exactly which data\n",
      "Audio File: segment_017_0206878_0220799.wav\n",
      "\n",
      "Segment segment_048:\n",
      "Text: what the program does and the outcome but doesn't care about things like control flow this style of programming is often associated with functional languages like haskell the other paradigm is imperative programming where your code uses statements like if and\n",
      "Audio File: segment_048_0588399_0602879.wav\n",
      "\n",
      "Segment segment_050:\n",
      "Text: means they support all these options at the same time in addition to object-oriented programming the idea behind oop is that you use classes to write a blueprint for the data or objects in your code a class can\n",
      "Audio File: segment_050_0613039_0626240.wav\n",
      "\n",
      "Searching for: can you explain the main topic?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aded546ebce748bcb76368909153ef32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "Segment segment_002:\n",
      "Text: [Music] welcome to computer science 101 in today's video you'll learn the science behind the garbage code you've been writing by learning 101 different computer science terms and concepts this is a computer it's just a piece of tape\n",
      "Audio File: segment_002_0023519_0037679.wav\n",
      "\n",
      "Segment segment_009:\n",
      "Text: applications for that computers have random access memory or ram it's like a neighborhood and inside of every house lives a byte every location has a memory address which the cpu can read and write to you can think of the cpu and ram as\n",
      "Audio File: segment_009_0108640_0123359.wav\n",
      "\n",
      "Segment segment_052:\n",
      "Text: inheritance where a subclass can extend and override the behaviors of the parent class and it opens the door to all kinds of other ideas called design patterns now a class by itself doesn't actually do anything instead it's used to\n",
      "Audio File: segment_052_0635519_0649519.wav\n",
      "\n",
      "Segment segment_004:\n",
      "Text: central processing unit if we crack it open we find a piece of silicon that contains billions of tiny transistors which are like microscopic on off switches the value at one of these switches is called a bit and is the smallest piece of information a computer\n",
      "Audio File: segment_004_0048479_0063280.wav\n",
      "\n",
      "Segment segment_011:\n",
      "Text: together because we have operating system kernels like linux mac and windows that control all hardware resources via device drivers now to start hacking on the operating system your first entry point is the shell which is a program that exposes the\n",
      "Audio File: segment_011_0133520_0148400.wav\n",
      "\n",
      "Searching for: tell me more about the details\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8805061213824670bb466755dd03eb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "Segment segment_011:\n",
      "Text: together because we have operating system kernels like linux mac and windows that control all hardware resources via device drivers now to start hacking on the operating system your first entry point is the shell which is a program that exposes the\n",
      "Audio File: segment_011_0133520_0148400.wav\n",
      "\n",
      "Segment segment_001:\n",
      "Text: learn to code and get a high paying job while literally having no idea how anything actually works it all just feels like magic like a pilot driving a giant metal tube in the sky while knowing nothing about aerodynamics\n",
      "Audio File: segment_001_0011279_0024719.wav\n",
      "\n",
      "Segment segment_009:\n",
      "Text: applications for that computers have random access memory or ram it's like a neighborhood and inside of every house lives a byte every location has a memory address which the cpu can read and write to you can think of the cpu and ram as\n",
      "Audio File: segment_009_0108640_0123359.wav\n",
      "\n",
      "Segment segment_060:\n",
      "Text: address is usually alias to a url that is registered in a global database called the domain name service now to establish a connection the two computers will perform a tcp handshake which will allow them to exchange messages called\n",
      "Audio File: segment_060_0732240_0746958.wav\n",
      "\n",
      "Segment segment_061:\n",
      "Text: packets on top of that there's usually a security layer like ssl to encrypt and decrypt the messages over the network now the two computers can securely share data with the hypertext transfer protocol the client may request a web\n",
      "Audio File: segment_061_0744799_0758959.wav\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import os\n",
    "import time\n",
    "\n",
    "class SimpleRetriever:\n",
    "    def __init__(self, audio_dir: str = \"segments\"):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.db = lancedb.connect(\"segments.db\")\n",
    "        self.audio_dir = audio_dir\n",
    "        \n",
    "    def search(self, query: str, limit: int = 5):\n",
    "        table = self.db.open_table(\"segments\")\n",
    "        query_vector = self.model.encode(query).tolist()\n",
    "        \n",
    "        results = table.search(\n",
    "            query_vector,\n",
    "            vector_column_name=\"vector\"\n",
    "        ).limit(limit).to_list()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def play_audio(self, audio_filename: str):\n",
    "        \"\"\"Play audio file and wait for it to finish\"\"\"\n",
    "        try:\n",
    "            # Construct full path to audio file\n",
    "            audio_path = os.path.join(self.audio_dir, audio_filename)\n",
    "            \n",
    "            # Load the audio file\n",
    "            data, samplerate = sf.read(audio_path)\n",
    "            \n",
    "            # Play the audio\n",
    "            print(f\"\\nPlaying audio: {audio_filename}\")\n",
    "            sd.play(data, samplerate)\n",
    "            \n",
    "            # Wait until the audio is finished playing\n",
    "            status = sd.wait()\n",
    "            \n",
    "            if status:\n",
    "                print(\"Audio playback completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error playing audio: {str(e)}\")\n",
    "\n",
    "def test_retriever():\n",
    "    # Initialize retriever\n",
    "    retriever = SimpleRetriever()\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"what is this about?\",\n",
    "        \"can you explain the main topic?\",\n",
    "        \"tell me more about the details\"\n",
    "    ]\n",
    "    \n",
    "    # Run searches\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nSearching for: {query}\")\n",
    "        results = retriever.search(query)\n",
    "        \n",
    "        print(\"Results:\")\n",
    "        for r in results:\n",
    "            print(f\"\\nSegment {r['id']}:\")\n",
    "            print(f\"Text: {r['text']}\")\n",
    "            print(f\"Audio File: {r['audio_file']}\")\n",
    "            \n",
    "            # Ask if user wants to play the audio\n",
    "            play = input(f\"Would you like to play this audio segment? (y/n): \")\n",
    "            if play.lower() == 'y':\n",
    "                retriever.play_audio(r['audio_file'])\n",
    "                # Small pause between audio segments\n",
    "                time.sleep(1)\n",
    "\n",
    "def interactive_search():\n",
    "    \"\"\"Interactive search function that allows users to search and play audio\"\"\"\n",
    "    retriever = SimpleRetriever()\n",
    "    \n",
    "    while True:\n",
    "        # Get search query from user\n",
    "        query = input(\"\\nEnter your search query (or 'quit' to exit): \")\n",
    "        \n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        # Perform search\n",
    "        results = retriever.search(query)\n",
    "        \n",
    "        # Display results\n",
    "        for i, r in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. Segment {r['id']}:\")\n",
    "            print(f\"Text: {r['text']}\")\n",
    "            print(f\"Audio File: {r['audio_file']}\")\n",
    "        \n",
    "        # Ask which result to play\n",
    "        while True:\n",
    "            choice = input(\"\\nEnter the number of the segment to play (or 'n' for next search): \")\n",
    "            \n",
    "            if choice.lower() == 'n':\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                choice_idx = int(choice) - 1\n",
    "                if 0 <= choice_idx < len(results):\n",
    "                    retriever.play_audio(results[choice_idx]['audio_file'])\n",
    "                else:\n",
    "                    print(\"Invalid selection. Please try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number or 'n'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"1. Run test queries\")\n",
    "    print(\"2. Interactive search\")\n",
    "    choice = input(\"Select mode (1 or 2): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        test_retriever()\n",
    "    elif choice == \"2\":\n",
    "        interactive_search()\n",
    "    else:\n",
    "        print(\"Invalid choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Run test queries\n",
      "2. Interactive search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6613a533d845499df08ead14f91e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Segment segment_000:\n",
      "Text: what's the first thing you should do when your code throws an error obviously you should change nothing and try to run it again a few times if that doesn't work you're gonna need a computer science degree the awesome thing about software engineering is that you can\n",
      "Audio File: segment_000_0000080_0013359.wav\n",
      "\n",
      "2. Segment segment_041:\n",
      "Text: memory for executing your code when a function keeps calling itself the language will keep pushing frames onto the call stack until you get a stack overflow error to avoid this your algorithm needs a base condition so it knows when to terminate the loop now\n",
      "Audio File: segment_041_0499839_0513438.wav\n",
      "\n",
      "3. Segment segment_038:\n",
      "Text: while loop will run this block of code over and over again until the condition in the parentheses becomes false that can be useful but more often than not you'll want to loop over an iterable data type like an array most languages have a for loop that can run some code\n",
      "Audio File: segment_038_0461759_0476639.wav\n",
      "\n",
      "4. Segment segment_036:\n",
      "Text: boolean data type and whenever your code produces a value like this it's known as an expression but not all code will produce a value sometimes your code will simply do something which is known as a statement a good example is the if\n",
      "Audio File: segment_036_0438000_0451360.wav\n",
      "\n",
      "5. Segment segment_053:\n",
      "Text: instantiate objects which are actual chunks of data that live in your computer's memory often you'll want to reference the same object over and over again in your code when data is long-lived it can't go in the call stack instead most languages have a separate\n",
      "Audio File: segment_053_0647519_0661839.wav\n",
      "\n",
      "Playing audio: segment_000_0000080_0013359.wav\n",
      "Invalid input. Please enter a number or 'n'.\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import os\n",
    "import time\n",
    "\n",
    "class SimpleRetriever:\n",
    "    def __init__(self, audio_dir: str = \"segments\"):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.db = lancedb.connect(\"segments.db\")\n",
    "        self.audio_dir = audio_dir\n",
    "        \n",
    "    def search(self, query: str, limit: int = 5):\n",
    "        table = self.db.open_table(\"segments\")\n",
    "        query_vector = self.model.encode(query).tolist()\n",
    "        \n",
    "        results = table.search(\n",
    "            query_vector,\n",
    "            vector_column_name=\"vector\"\n",
    "        ).limit(limit).to_list()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def play_audio(self, audio_filename: str):\n",
    "        \"\"\"Play audio file and wait for it to finish\"\"\"\n",
    "        try:\n",
    "            # Construct full path to audio file\n",
    "            audio_path = os.path.join(self.audio_dir, audio_filename)\n",
    "            \n",
    "            # Load the audio file\n",
    "            data, samplerate = sf.read(audio_path)\n",
    "            \n",
    "            # Play the audio\n",
    "            print(f\"\\nPlaying audio: {audio_filename}\")\n",
    "            sd.play(data, samplerate)\n",
    "            \n",
    "            # Wait until the audio is finished playing\n",
    "            status = sd.wait()\n",
    "            \n",
    "            if status:\n",
    "                print(\"Audio playback completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error playing audio: {str(e)}\")\n",
    "\n",
    "def test_retriever():\n",
    "    # Initialize retriever\n",
    "    retriever = SimpleRetriever()\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"what is this about?\",\n",
    "        \"can you explain the main topic?\",\n",
    "        \"tell me more about the details\"\n",
    "    ]\n",
    "    \n",
    "    # Run searches\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nSearching for: {query}\")\n",
    "        results = retriever.search(query)\n",
    "        \n",
    "        print(\"Results:\")\n",
    "        for r in results:\n",
    "            print(f\"\\nSegment {r['id']}:\")\n",
    "            print(f\"Text: {r['text']}\")\n",
    "            print(f\"Audio File: {r['audio_file']}\")\n",
    "            \n",
    "            # Ask if user wants to play the audio\n",
    "            play = input(f\"Would you like to play this audio segment? (y/n): \")\n",
    "            if play.lower() == 'y':\n",
    "                retriever.play_audio(r['audio_file'])\n",
    "                # Small pause between audio segments\n",
    "                time.sleep(1)\n",
    "\n",
    "def interactive_search():\n",
    "    \"\"\"Interactive search function that allows users to search and play audio\"\"\"\n",
    "    retriever = SimpleRetriever()\n",
    "    \n",
    "    while True:\n",
    "        # Get search query from user\n",
    "        query = input(\"\\nEnter your search query (or 'quit' to exit): \")\n",
    "        \n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        # Perform search\n",
    "        results = retriever.search(query)\n",
    "        \n",
    "        # Display results\n",
    "        for i, r in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. Segment {r['id']}:\")\n",
    "            print(f\"Text: {r['text']}\")\n",
    "            print(f\"Audio File: {r['audio_file']}\")\n",
    "        \n",
    "        # Ask which result to play\n",
    "        while True:\n",
    "            choice = input(\"\\nEnter the number of the segment to play (or 'n' for next search): \")\n",
    "            \n",
    "            if choice.lower() == 'n':\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                choice_idx = int(choice) - 1\n",
    "                if 0 <= choice_idx < len(results):\n",
    "                    retriever.play_audio(results[choice_idx]['audio_file'])\n",
    "                else:\n",
    "                    print(\"Invalid selection. Please try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number or 'n'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"1. Run test queries\")\n",
    "    print(\"2. Interactive search\")\n",
    "    choice = input(\"Select mode (1 or 2): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        test_retriever()\n",
    "    elif choice == \"2\":\n",
    "        interactive_search()\n",
    "    else:\n",
    "        print(\"Invalid choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
